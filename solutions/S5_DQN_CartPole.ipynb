{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "S5_DQN_CartPole.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pabair/rl-course-ss21/blob/main/solutions/S5_DQN_CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCLrRFHSKl_5"
      },
      "source": [
        "# Deep Q-Network with Cart Pole\n",
        "\n",
        "This notebook shows an implementation of a DQN on the Cart Pole environment.\n",
        "\n",
        "Note: The following code is heavily inspired by [this]( https://www.katnoria.com/nb_dqn_lunar/) blog post.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2RNqaAGiLU1"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "We first need to install some dependencies for using the environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZXskDwXKl_-"
      },
      "source": [
        "import random\n",
        "import sys\n",
        "from time import time\n",
        "from collections import deque, defaultdict, namedtuple\n",
        "import numpy as np\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVO0INWR1DYS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cd5ba49-dfe0-488d-b4e8-f79e78df4835"
      },
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "env.seed(0)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syEtLpus_2hx",
        "outputId": "59210e3f-91c5-4217-9bf0-dce2a427252e"
      },
      "source": [
        "print(env.action_space.n)\n",
        "print(env.observation_space.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "(4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lrq9VwzO1Zx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5587f91e-a059-49b9-cf8b-f08776bfbe52"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKg3BvSnivPE"
      },
      "source": [
        "## 2. Define the neural network, the replay buffer and the agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9pG_Ii7jToR"
      },
      "source": [
        "First, we define the neural network that predicts the Q-values for all actions, given a state as input.\n",
        "This is a fully-connected neural net with two hidden layers using Relu activations.\n",
        "The last layer does not have any activation and outputs a Q-value for every action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFxqeLkf1eHY"
      },
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, 32)\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        self.fc3 = nn.Linear(64, action_size)  \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)     "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0xHaPTIj1pD"
      },
      "source": [
        "Next, we define a replay buffer that saves previous transitions (so-called `experiences`) and provides a `sample` function to randomly extract a batch of experiences from the buffer.\n",
        "\n",
        "Note that experiences are internally saved as `numpy`-arrays. They are converted back to PyTorch tensors before being returned by the `sample`-method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQw6QVAC1hQf"
      },
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size, batch_size, seed):\n",
        "        self.batch_size = batch_size\n",
        "        self.seed = random.seed(seed)\n",
        "        self.memory = deque(maxlen=buffer_size) # maximum size of buffer\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        experience = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(experience)\n",
        "                \n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, self.batch_size)\n",
        "        \n",
        "        # Convert to PyTorch tensors\n",
        "        states = np.vstack([experience.state for experience in experiences if experience is not None])\n",
        "        states_tensor = torch.from_numpy(states).float().to(device)\n",
        "\n",
        "        actions = np.vstack([experience.action for experience in experiences if experience is not None])\n",
        "        actions_tensor = torch.from_numpy(actions).long().to(device)\n",
        "\n",
        "        rewards = np.vstack([experience.reward for experience in experiences if experience is not None])\n",
        "        rewards_tensor = torch.from_numpy(rewards).float().to(device)\n",
        "\n",
        "        next_states = np.vstack([experience.next_state for experience in experiences if experience is not None])\n",
        "        next_states_tensor = torch.from_numpy(next_states).float().to(device)\n",
        "        \n",
        "        # Convert done flag from boolean to int\n",
        "        dones = np.vstack([experience.done for experience in experiences if experience is not None]).astype(np.uint8)\n",
        "        dones_tensor = torch.from_numpy(dones).float().to(device)\n",
        "        \n",
        "        return (states_tensor, actions_tensor, rewards_tensor, next_states_tensor, dones_tensor)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYjlS7Fy1jJA"
      },
      "source": [
        "BUFFER_SIZE = int(1e5)  # Replay memory size\n",
        "BATCH_SIZE = 64         # Number of experiences to sample from memory\n",
        "GAMMA = 0.99            # Discount factor\n",
        "TAU = 1e-3              # Soft update parameter for updating fixed q network\n",
        "LR = 1e-4               # Q Network learning rate\n",
        "UPDATE_EVERY = 4        # How often to update Q network\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        # Initialize Q and Fixed Q networks\n",
        "        self.q_network = QNetwork(state_size, action_size, seed).to(device)\n",
        "        self.fixed_network = QNetwork(state_size, action_size, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters())\n",
        "        # Initiliase memory \n",
        "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "        self.timestep = 0\n",
        "    \n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "        self.timestep += 1\n",
        "        \n",
        "        # trigger training\n",
        "        if self.timestep % UPDATE_EVERY == 0:\n",
        "            if len(self.memory) > BATCH_SIZE: # only when buffer is filled\n",
        "                sampled_experiences = self.memory.sample()\n",
        "                self.learn(sampled_experiences) \n",
        "        \n",
        "    def learn(self, experiences):\n",
        " \n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        action_values = self.fixed_network(next_states).detach()\n",
        "        max_action_values = action_values.max(1)[0].unsqueeze(1)\n",
        "        \n",
        "        # If \"done\" just use reward, else update Q_target with discounted action values\n",
        "        Q_target = rewards + (GAMMA * max_action_values * (1 - dones))\n",
        "        Q_expected = self.q_network(states).gather(1, actions)\n",
        "\n",
        "        # Calculate loss and update weights\n",
        "        loss = F.mse_loss(Q_expected, Q_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        # Update fixed weights\n",
        "        self.update_fixed_network(self.q_network, self.fixed_network)\n",
        "        \n",
        "    def update_fixed_network(self, q_network, fixed_network):\n",
        "        for source_parameters, target_parameters in zip(q_network.parameters(), fixed_network.parameters()):\n",
        "            target_parameters.data.copy_(TAU * source_parameters.data + (1.0 - TAU) * target_parameters.data)\n",
        "        \n",
        "        \n",
        "    def act(self, state, eps=0.0):\n",
        "\n",
        "        rnd = random.random()\n",
        "        if rnd < eps:\n",
        "            return np.random.randint(self.action_size)\n",
        "        else:\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "            action_values = self.q_network(state)\n",
        "            action = np.argmax(action_values.cpu().data.numpy())\n",
        "            return action"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P3-UIm0fh3W"
      },
      "source": [
        "### 3. Executes episodes and train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NikAZhjNfsoi"
      },
      "source": [
        "We first define some paramters which are guiding the training process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJGrZry81pu4"
      },
      "source": [
        "MAX_EPISODES = 2000  # Max number of episodes to play\n",
        "MAX_STEPS = 1000     # Max steps allowed in a single episode/play\n",
        "\n",
        "# Epsilon schedule\n",
        "EPS_START = 1.0      # Default/starting value of eps\n",
        "EPS_DECAY = 0.999    # Epsilon decay rate\n",
        "EPS_MIN = 0.01       # Minimum epsilon "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezOn9IpKf17C"
      },
      "source": [
        "Then we start executing episodes and observe the mean score per episode.\n",
        "The environment is considered as solved if this score is above 200."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_EC7XLJ1slY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ec5f09a-f4ae-4284-b457-79004da5cda6"
      },
      "source": [
        "# Get state and action sizes\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "print('State size: {}, action size: {}'.format(state_size, action_size))\n",
        "dqn_agent = DQNAgent(state_size, action_size, seed=0)\n",
        "start = time()\n",
        "\n",
        "# Maintain a list of last 100 scores\n",
        "scores_window = deque(maxlen=100)\n",
        "eps = EPS_START\n",
        "for episode in range(1, MAX_EPISODES + 1):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    for t in range(MAX_STEPS):\n",
        "        action = dqn_agent.act(state, eps)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        dqn_agent.step(state, action, reward, next_state, done)\n",
        "        state = next_state        \n",
        "        score += reward        \n",
        "        if done:\n",
        "            break\n",
        "            \n",
        "        eps = max(eps * EPS_DECAY, EPS_MIN)\n",
        "\n",
        "    scores_window.append(score)\n",
        "\n",
        "    if episode % 99 == 0:\n",
        "        mean_score = np.mean(scores_window)\n",
        "        print('Progress {}/{}, average score:{:.2f}'.format(episode, MAX_EPISODES, mean_score))\n",
        "\n",
        "    mean_score = np.mean(scores_window)\n",
        "    if mean_score >= 200:\n",
        "        print('\\rEnvironment solved in {} episodes, average score: {:.2f}'.format(episode, mean_score))\n",
        "        sys.stdout.flush()\n",
        "        break\n",
        "            \n",
        "end = time()    \n",
        "print('Took {} seconds'.format(end - start))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State size: 4, action size: 2\n",
            "Progress 99/2000, average score:13.65\n",
            "Progress 198/2000, average score:10.60\n",
            "Progress 297/2000, average score:9.68\n",
            "Progress 396/2000, average score:9.67\n",
            "Progress 495/2000, average score:9.54\n",
            "Progress 594/2000, average score:9.40\n",
            "Progress 693/2000, average score:9.67\n",
            "Progress 792/2000, average score:11.09\n",
            "Progress 891/2000, average score:10.72\n",
            "Progress 990/2000, average score:10.97\n",
            "Progress 1089/2000, average score:14.88\n",
            "Progress 1188/2000, average score:29.49\n",
            "Progress 1287/2000, average score:129.15\n",
            "Progress 1386/2000, average score:184.90\n",
            "Progress 1485/2000, average score:170.18\n",
            "Progress 1584/2000, average score:173.48\n",
            "Progress 1683/2000, average score:176.79\n",
            "Progress 1782/2000, average score:165.41\n",
            "Progress 1881/2000, average score:155.59\n",
            "Progress 1980/2000, average score:147.86\n",
            "Took 270.2958333492279 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd7QnYQRVUFc"
      },
      "source": [
        "### 4. Play epsiode and record it\n",
        "\n",
        "The following code enables Colab to record sessions (not needed when using executing code locally)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f3gVKzJoFss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a22690a-885b-4c8d-ca2f-df72552a80c2"
      },
      "source": [
        "!apt-get install -y xvfb x11-utils\n",
        "\n",
        "!pip install pyvirtualdisplay==0.2.* \\\n",
        "             PyOpenGL==3.1.* \\\n",
        "             PyOpenGL-accelerate==3.1.*\n",
        "\n",
        "!pip install gym[box2d]==0.17.*\n",
        "\n",
        "import pyvirtualdisplay\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
        "_ = _display.start()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libxxf86dga1\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  libxxf86dga1 x11-utils xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 993 kB of archives.\n",
            "After this operation, 2,981 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 993 kB in 2s (504 kB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "(Reading database ... 145483 files and directories currently installed.)\n",
            "Preparing to unpack .../libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../x11-utils_7.7+3build1_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+3build1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Setting up x11-utils (7.7+3build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting pyvirtualdisplay==0.2.*\n",
            "  Downloading https://files.pythonhosted.org/packages/69/ec/8221a07850d69fa3c57c02e526edd23d18c7c05d58ed103e3b19172757c1/PyVirtualDisplay-0.2.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.6/dist-packages (3.1.5)\n",
            "Collecting PyOpenGL-accelerate==3.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/3c/f42a62b7784c04b20f8b88d6c8ad04f4f20b0767b721102418aad94d8389/PyOpenGL-accelerate-3.1.5.tar.gz (538kB)\n",
            "\u001b[K     |████████████████████████████████| 542kB 4.3MB/s \n",
            "\u001b[?25hCollecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: PyOpenGL-accelerate\n",
            "  Building wheel for PyOpenGL-accelerate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyOpenGL-accelerate: filename=PyOpenGL_accelerate-3.1.5-cp36-cp36m-linux_x86_64.whl size=1593637 sha256=4299bd55c68fda6ca5b4a23598a8b9a750596719360a8c789a6cead3e3d6aba5\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/21/77/99670ceca25fddb3c2b60a7ae44644b8253d1006e8ec417bcc\n",
            "Successfully built PyOpenGL-accelerate\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay, PyOpenGL-accelerate\n",
            "Successfully installed EasyProcess-0.3 PyOpenGL-accelerate-3.1.5 pyvirtualdisplay-0.2.5\n",
            "Requirement already satisfied: gym[box2d]==0.17.* in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n",
            "Collecting box2d-py~=2.3.5; extra == \"box2d\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.16.0)\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gki1NW8sVlyd"
      },
      "source": [
        "Use the trained model to play and record one episode. The recorded video will be stored into the `video`-subfolder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-KWsd02TRZq",
        "outputId": "9bbd5543-8616-4d74-d302-92126a22a4c6"
      },
      "source": [
        "import time\n",
        "\n",
        "FPS = 25\n",
        "record_folder=\"video\"  \n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "env = gym.wrappers.Monitor(env, record_folder, force=True)\n",
        "\n",
        "state = env.reset()\n",
        "total_reward = 0.0\n",
        "\n",
        "while True:\n",
        "        start_ts = time.time()\n",
        "        env.render()\n",
        "        \n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        action_values = dqn_agent.q_network(state)\n",
        "        action = np.argmax(action_values.cpu().data.numpy())\n",
        "\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "            \n",
        "        delta = 1/FPS - (time.time() - start_ts)\n",
        "        if delta > 0:\n",
        "            time.sleep(delta)\n",
        "\n",
        "print(\"Total reward: %.2f\" % total_reward)\n",
        "env.close()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total reward: 120.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS67QgiUU9U7"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}