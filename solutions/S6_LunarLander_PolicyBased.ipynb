{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab6_LunarLander_CEM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pabair/rl-course-ss21/blob/main/solutions/S6_LunarLander_PolicyBased.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCLrRFHSKl_5"
      },
      "source": [
        "# Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGfIH5l2xZbu"
      },
      "source": [
        "# source: https://medium.com/coinmonks/landing-a-rocket-with-simple-reinforcement-learning-3a0265f8b58c"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96dExX1TKm2m",
        "outputId": "d879d222-b855-4ece-9f3e-f50b9f1cdb0a"
      },
      "source": [
        "!pip3 install box2d-py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting box2d-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 4.9MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZXskDwXKl_-"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "\n",
        "torch.manual_seed(1)\n",
        "np.random.seed(1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhZ0fzBkKmAA"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWQr7TZgKmAB"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, obs_size, hidden_size, n_actions):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(obs_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, n_actions)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zFMlVViKmAE"
      },
      "source": [
        "# Generate Episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIiayltZKmAF"
      },
      "source": [
        "def generate_batch(env, batch_size, t_max=5000):\n",
        "    \n",
        "    activation = nn.Softmax(dim=1)\n",
        "    batch_actions,batch_states, batch_rewards = [],[],[]\n",
        "    \n",
        "    for b in range(batch_size):\n",
        "        states,actions = [],[]\n",
        "        total_reward = 0\n",
        "        s = env.reset()\n",
        "        for t in range(t_max):\n",
        "            \n",
        "            s_v = torch.FloatTensor([s])\n",
        "            act_probs_v = activation(net(s_v))\n",
        "            act_probs = act_probs_v.data.numpy()[0]\n",
        "            a = np.random.choice(len(act_probs), p=act_probs)\n",
        "\n",
        "            new_s, r, done, info = env.step(a)\n",
        "\n",
        "            #record sessions like you did before\n",
        "            states.append(s)\n",
        "            actions.append(a)\n",
        "            total_reward += r\n",
        "\n",
        "            s = new_s\n",
        "            if done:\n",
        "                batch_actions.append(actions)\n",
        "                batch_states.append(states)\n",
        "                batch_rewards.append(total_reward)\n",
        "                break\n",
        "                \n",
        "    return batch_states, batch_actions, batch_rewards"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvq2ZIvlKmAJ"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnwhX7xOqe9r"
      },
      "source": [
        "def filter_batch(states_batch, actions_batch, rewards_batch, percentile):\n",
        "    \n",
        "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
        "    \n",
        "    elite_states = []\n",
        "    elite_actions = []\n",
        "    \n",
        "    \n",
        "    for i in range(len(rewards_batch)):\n",
        "        if rewards_batch[i] > reward_threshold:\n",
        "            for j in range(len(states_batch[i])):\n",
        "                elite_states.append(states_batch[i][j])\n",
        "                elite_actions.append(actions_batch[i][j])\n",
        "    \n",
        "    return elite_states, elite_actions"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFUzEnaDKmAJ",
        "outputId": "0a27eb7b-6cb4-45ee-b963-92cfb547170a"
      },
      "source": [
        "batch_size = 100\n",
        "session_size = 500\n",
        "percentile = 80\n",
        "hidden_size = 200\n",
        "completion_score = 100\n",
        "learning_rate = 0.01\n",
        "\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "n_states = env.observation_space.shape[0]\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "#neural network\n",
        "net = Net(n_states, hidden_size, n_actions)\n",
        "#loss function\n",
        "objective = nn.CrossEntropyLoss()\n",
        "#optimisation function\n",
        "optimizer = optim.Adam(params=net.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(session_size):\n",
        "    #generate new sessions\n",
        "    batch_states, batch_actions, batch_rewards = generate_batch(env, batch_size, t_max=500)\n",
        "\n",
        "    elite_states, elite_actions = filter_batch(batch_states, batch_actions, batch_rewards, percentile)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    tensor_states = torch.FloatTensor(elite_states)\n",
        "    tensor_actions = torch.LongTensor(elite_actions)\n",
        "    action_scores_v = net(tensor_states)\n",
        "    loss_v = objective(action_scores_v, tensor_actions)\n",
        "    loss_v.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #show results\n",
        "    mean_reward, threshold = np.mean(batch_rewards), np.percentile(batch_rewards, percentile)\n",
        "    print(\"%d: loss=%.3f, reward_mean=%.1f, reward_threshold=%.1f\" % (\n",
        "            i, loss_v.item(), mean_reward, threshold))\n",
        "    \n",
        "    #check if \n",
        "    if np.mean(batch_rewards)> completion_score:\n",
        "        print(\"Environment has been successfullly completed!\")\n",
        "        break\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: loss=1.387, reward_mean=-188.8, reward_threshold=-87.9\n",
            "1: loss=1.368, reward_mean=-242.0, reward_threshold=-119.4\n",
            "2: loss=1.350, reward_mean=-227.0, reward_threshold=-99.5\n",
            "3: loss=1.323, reward_mean=-201.6, reward_threshold=-101.7\n",
            "4: loss=1.303, reward_mean=-161.2, reward_threshold=-82.3\n",
            "5: loss=1.274, reward_mean=-131.2, reward_threshold=-86.0\n",
            "6: loss=1.270, reward_mean=-131.8, reward_threshold=-78.2\n",
            "7: loss=1.246, reward_mean=-118.1, reward_threshold=-77.3\n",
            "8: loss=1.221, reward_mean=-107.6, reward_threshold=-59.0\n",
            "9: loss=1.189, reward_mean=-94.6, reward_threshold=-54.7\n",
            "10: loss=1.164, reward_mean=-99.1, reward_threshold=-57.4\n",
            "11: loss=1.134, reward_mean=-99.3, reward_threshold=-39.6\n",
            "12: loss=1.116, reward_mean=-89.9, reward_threshold=-20.7\n",
            "13: loss=1.108, reward_mean=-151.7, reward_threshold=-28.3\n",
            "14: loss=1.102, reward_mean=-180.5, reward_threshold=-54.3\n",
            "15: loss=1.089, reward_mean=-146.1, reward_threshold=-37.9\n",
            "16: loss=1.063, reward_mean=-113.2, reward_threshold=-20.9\n",
            "17: loss=1.099, reward_mean=-80.1, reward_threshold=-16.3\n",
            "18: loss=1.089, reward_mean=-62.6, reward_threshold=-1.9\n",
            "19: loss=1.082, reward_mean=-49.0, reward_threshold=3.4\n",
            "20: loss=1.070, reward_mean=-43.5, reward_threshold=4.9\n",
            "21: loss=1.075, reward_mean=-28.0, reward_threshold=10.4\n",
            "22: loss=1.068, reward_mean=-31.2, reward_threshold=5.1\n",
            "23: loss=1.066, reward_mean=-29.1, reward_threshold=14.4\n",
            "24: loss=1.050, reward_mean=-16.9, reward_threshold=15.0\n",
            "25: loss=1.054, reward_mean=-13.9, reward_threshold=19.5\n",
            "26: loss=1.046, reward_mean=-6.2, reward_threshold=25.9\n",
            "27: loss=1.040, reward_mean=-29.9, reward_threshold=18.0\n",
            "28: loss=1.032, reward_mean=-8.3, reward_threshold=23.3\n",
            "29: loss=1.035, reward_mean=-6.4, reward_threshold=25.9\n",
            "30: loss=0.996, reward_mean=9.1, reward_threshold=35.7\n",
            "31: loss=1.025, reward_mean=10.8, reward_threshold=36.9\n",
            "32: loss=1.014, reward_mean=9.4, reward_threshold=35.9\n",
            "33: loss=1.000, reward_mean=6.5, reward_threshold=31.5\n",
            "34: loss=1.021, reward_mean=13.2, reward_threshold=34.7\n",
            "35: loss=1.030, reward_mean=7.5, reward_threshold=31.5\n",
            "36: loss=0.952, reward_mean=7.1, reward_threshold=32.6\n",
            "37: loss=0.947, reward_mean=11.7, reward_threshold=37.1\n",
            "38: loss=0.998, reward_mean=0.2, reward_threshold=31.5\n",
            "39: loss=0.926, reward_mean=-9.3, reward_threshold=39.0\n",
            "40: loss=0.948, reward_mean=-1.2, reward_threshold=36.1\n",
            "41: loss=0.993, reward_mean=3.3, reward_threshold=42.4\n",
            "42: loss=0.943, reward_mean=19.8, reward_threshold=46.0\n",
            "43: loss=0.954, reward_mean=19.4, reward_threshold=42.1\n",
            "44: loss=0.964, reward_mean=12.3, reward_threshold=43.3\n",
            "45: loss=0.906, reward_mean=11.2, reward_threshold=39.9\n",
            "46: loss=0.961, reward_mean=5.5, reward_threshold=37.7\n",
            "47: loss=0.879, reward_mean=10.2, reward_threshold=31.2\n",
            "48: loss=0.929, reward_mean=6.6, reward_threshold=30.7\n",
            "49: loss=0.883, reward_mean=10.0, reward_threshold=36.5\n",
            "50: loss=0.868, reward_mean=9.7, reward_threshold=41.0\n",
            "51: loss=0.909, reward_mean=9.2, reward_threshold=39.6\n",
            "52: loss=0.873, reward_mean=12.6, reward_threshold=39.1\n",
            "53: loss=0.796, reward_mean=14.4, reward_threshold=41.9\n",
            "54: loss=0.785, reward_mean=0.2, reward_threshold=31.2\n",
            "55: loss=0.752, reward_mean=16.9, reward_threshold=40.4\n",
            "56: loss=0.781, reward_mean=15.1, reward_threshold=42.1\n",
            "57: loss=0.726, reward_mean=25.7, reward_threshold=45.0\n",
            "58: loss=0.742, reward_mean=28.4, reward_threshold=45.9\n",
            "59: loss=0.785, reward_mean=31.0, reward_threshold=50.5\n",
            "60: loss=0.768, reward_mean=22.4, reward_threshold=44.8\n",
            "61: loss=0.758, reward_mean=31.1, reward_threshold=50.6\n",
            "62: loss=0.683, reward_mean=20.2, reward_threshold=40.4\n",
            "63: loss=0.733, reward_mean=20.2, reward_threshold=39.6\n",
            "64: loss=0.613, reward_mean=40.4, reward_threshold=70.2\n",
            "65: loss=0.617, reward_mean=45.4, reward_threshold=84.4\n",
            "66: loss=0.682, reward_mean=54.9, reward_threshold=173.3\n",
            "67: loss=0.529, reward_mean=95.9, reward_threshold=232.2\n",
            "68: loss=0.566, reward_mean=60.4, reward_threshold=213.1\n",
            "69: loss=0.575, reward_mean=87.9, reward_threshold=222.7\n",
            "70: loss=0.616, reward_mean=84.2, reward_threshold=208.9\n",
            "71: loss=0.582, reward_mean=95.3, reward_threshold=224.6\n",
            "72: loss=0.559, reward_mean=82.1, reward_threshold=216.2\n",
            "73: loss=0.623, reward_mean=58.1, reward_threshold=205.2\n",
            "74: loss=0.657, reward_mean=90.9, reward_threshold=218.2\n",
            "75: loss=0.614, reward_mean=90.5, reward_threshold=206.7\n",
            "76: loss=0.568, reward_mean=98.1, reward_threshold=216.0\n",
            "77: loss=0.671, reward_mean=109.6, reward_threshold=216.0\n",
            "Environment has been successfullly completed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA06LBJ5JgZH"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUlnIIwC9AyV",
        "outputId": "4925a26c-9680-463b-934a-d4dd13455df1"
      },
      "source": [
        "!apt-get install -y xvfb x11-utils\n",
        "\n",
        "!pip install pyvirtualdisplay==0.2.* \\\n",
        "             PyOpenGL==3.1.* \\\n",
        "             PyOpenGL-accelerate==3.1.*\n",
        "\n",
        "!pip install gym[box2d]==0.17.*\n",
        "\n",
        "import pyvirtualdisplay\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
        "_ = _display.start()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libxxf86dga1\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  libxxf86dga1 x11-utils xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 13 not upgraded.\n",
            "Need to get 993 kB of archives.\n",
            "After this operation, 2,981 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 993 kB in 1s (1,197 kB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "(Reading database ... 146374 files and directories currently installed.)\n",
            "Preparing to unpack .../libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../x11-utils_7.7+3build1_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+3build1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Setting up x11-utils (7.7+3build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting pyvirtualdisplay==0.2.*\n",
            "  Downloading https://files.pythonhosted.org/packages/69/ec/8221a07850d69fa3c57c02e526edd23d18c7c05d58ed103e3b19172757c1/PyVirtualDisplay-0.2.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.6/dist-packages (3.1.5)\n",
            "Collecting PyOpenGL-accelerate==3.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/3c/f42a62b7784c04b20f8b88d6c8ad04f4f20b0767b721102418aad94d8389/PyOpenGL-accelerate-3.1.5.tar.gz (538kB)\n",
            "\u001b[K     |████████████████████████████████| 542kB 3.8MB/s \n",
            "\u001b[?25hCollecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: PyOpenGL-accelerate\n",
            "  Building wheel for PyOpenGL-accelerate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyOpenGL-accelerate: filename=PyOpenGL_accelerate-3.1.5-cp36-cp36m-linux_x86_64.whl size=1593655 sha256=64e3dd0118ec26fcad1f32f2381cde5270d4708711505ce9ee3d7d16d8683d12\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/21/77/99670ceca25fddb3c2b60a7ae44644b8253d1006e8ec417bcc\n",
            "Successfully built PyOpenGL-accelerate\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay, PyOpenGL-accelerate\n",
            "Successfully installed EasyProcess-0.3 PyOpenGL-accelerate-3.1.5 pyvirtualdisplay-0.2.5\n",
            "Requirement already satisfied: gym[box2d]==0.17.* in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (1.19.5)\n",
            "Requirement already satisfied: box2d-py~=2.3.5; extra == \"box2d\" in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (2.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2KaeLMD9Dx7",
        "outputId": "058823e0-b145-4917-d70a-28ba99f6a2d3"
      },
      "source": [
        "import time\n",
        "\n",
        "FPS = 25\n",
        "record_folder=\"video\"  \n",
        "\n",
        "env = gym.make('LunarLander-v2')\n",
        "env = gym.wrappers.Monitor(env, record_folder, force=True)\n",
        "\n",
        "state = env.reset()\n",
        "total_reward = 0.0\n",
        "\n",
        "activation = nn.Softmax(dim=1)\n",
        "\n",
        "while True:\n",
        "        start_ts = time.time()\n",
        "        env.render()\n",
        "           \n",
        "        s_v = torch.FloatTensor([state])\n",
        "        act_probs_v = activation(net(s_v))\n",
        "        act_probs = act_probs_v.data.numpy()[0]\n",
        "        a = np.random.choice(len(act_probs), p=act_probs)\n",
        "\n",
        "        state, reward, done, _ = env.step(a)\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "            \n",
        "        delta = 1/FPS - (time.time() - start_ts)\n",
        "        if delta > 0:\n",
        "            time.sleep(delta)\n",
        "\n",
        "print(\"Total reward: %.2f\" % total_reward)\n",
        "env.close()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total reward: 241.31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qip60oSbWJTI"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}