{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "5_DQN_LunarLander.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pabair/rl-course-ss21/blob/main/5_DQN_LunarLander.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCLrRFHSKl_5"
      },
      "source": [
        "# Deep Q-Network with Lunar Lander\n",
        "\n",
        "This notebook shows an implementation of a DQN on the LunarLander environment.\n",
        "Details on the environment can be found [here](https://gym.openai.com/envs/LunarLander-v2/).\n",
        "\n",
        "Note: The following code is heavily inspired by [this]( https://www.katnoria.com/nb_dqn_lunar/) blog post.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2RNqaAGiLU1"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "We first need to install some dependencies for using the environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96dExX1TKm2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a3a6d38-7cae-4c85-dc5f-c0182c2e8275"
      },
      "source": [
        "!pip3 install box2d-py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.6/dist-packages (2.3.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZXskDwXKl_-"
      },
      "source": [
        "import random\n",
        "import sys\n",
        "from time import time\n",
        "from collections import deque, defaultdict, namedtuple\n",
        "import numpy as np\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVO0INWR1DYS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68e453b9-79a7-4921-ac7c-186de15ec6c5"
      },
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "env.seed(0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lrq9VwzO1Zx4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae7034e1-9628-4794-bba6-f7dac7ad5de4"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKg3BvSnivPE"
      },
      "source": [
        "## 2. Define the neural network, the replay buffer and the agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9pG_Ii7jToR"
      },
      "source": [
        "First, we define the neural network that predicts the Q-values for all actions, given a state as input.\n",
        "This is a fully-connected neural net with two hidden layers using Relu activations.\n",
        "The last layer does not have any activation and outputs a Q-value for every action."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFxqeLkf1eHY"
      },
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, 32)\n",
        "        self.fc2 = nn.Linear(32, 64)\n",
        "        self.fc3 = nn.Linear(64, action_size)  \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)     "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0xHaPTIj1pD"
      },
      "source": [
        "Next, we define a replay buffer that saves previous transitions (so-called `experiences`) and provides a `sample` function to randomly extract a batch of experiences from the buffer.\n",
        "\n",
        "Note that experiences are internally saved as `numpy`-arrays. They are converted back to PyTorch tensors before being returned by the `sample`-method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQw6QVAC1hQf"
      },
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size, batch_size, seed):\n",
        "        self.batch_size = batch_size\n",
        "        self.seed = random.seed(seed)\n",
        "        self.memory = deque(maxlen=buffer_size) # maximum size of buffer\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        experience = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(experience)\n",
        "                \n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, self.batch_size)\n",
        "        \n",
        "        # Convert to PyTorch tensors\n",
        "        states = np.vstack([experience.state for experience in experiences if experience is not None])\n",
        "        states_tensor = torch.from_numpy(states).float().to(device)\n",
        "\n",
        "        actions = np.vstack([experience.action for experience in experiences if experience is not None])\n",
        "        actions_tensor = torch.from_numpy(actions).long().to(device)\n",
        "\n",
        "        rewards = np.vstack([experience.reward for experience in experiences if experience is not None])\n",
        "        rewards_tensor = torch.from_numpy(rewards).float().to(device)\n",
        "\n",
        "        next_states = np.vstack([experience.next_state for experience in experiences if experience is not None])\n",
        "        next_states_tensor = torch.from_numpy(next_states).float().to(device)\n",
        "        \n",
        "        # Convert done flag from boolean to int\n",
        "        dones = np.vstack([experience.done for experience in experiences if experience is not None]).astype(np.uint8)\n",
        "        dones_tensor = torch.from_numpy(dones).float().to(device)\n",
        "        \n",
        "        return (states_tensor, actions_tensor, rewards_tensor, next_states_tensor, dones_tensor)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYjlS7Fy1jJA"
      },
      "source": [
        "BUFFER_SIZE = int(1e5)  # Replay memory size\n",
        "BATCH_SIZE = 64         # Number of experiences to sample from memory\n",
        "GAMMA = 0.99            # Discount factor\n",
        "TAU = 1e-3              # Soft update parameter for updating fixed q network\n",
        "LR = 1e-4               # Q Network learning rate\n",
        "UPDATE_EVERY = 4        # How often to update Q network\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        # Initialize Q and Fixed Q networks\n",
        "        self.q_network = QNetwork(state_size, action_size, seed).to(device)\n",
        "        self.fixed_network = QNetwork(state_size, action_size, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters())\n",
        "        # Initiliase memory \n",
        "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "        self.timestep = 0\n",
        "    \n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "        self.timestep += 1\n",
        "        \n",
        "        # trigger training\n",
        "        if self.timestep % UPDATE_EVERY == 0:\n",
        "            if len(self.memory) > BATCH_SIZE: # only when buffer is filled\n",
        "                sampled_experiences = self.memory.sample()\n",
        "                self.learn(sampled_experiences) \n",
        "        \n",
        "    def learn(self, experiences):\n",
        " \n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        action_values = self.fixed_network(next_states).detach()\n",
        "        max_action_values = action_values.max(1)[0].unsqueeze(1)\n",
        "        \n",
        "        # If \"done\" just use reward, else update Q_target with discounted action values\n",
        "        Q_target = rewards + (GAMMA * max_action_values * (1 - dones))\n",
        "        Q_expected = self.q_network(states).gather(1, actions)\n",
        "\n",
        "        # Calculate loss and update weights\n",
        "        loss = F.mse_loss(Q_expected, Q_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        # Update fixed weights\n",
        "        self.update_fixed_network(self.q_network, self.fixed_network)\n",
        "        \n",
        "    def update_fixed_network(self, q_network, fixed_network):\n",
        "        for source_parameters, target_parameters in zip(q_network.parameters(), fixed_network.parameters()):\n",
        "            target_parameters.data.copy_(TAU * source_parameters.data + (1.0 - TAU) * target_parameters.data)\n",
        "        \n",
        "        \n",
        "    def act(self, state, eps=0.0):\n",
        "        rnd = random.random()\n",
        "        if rnd < eps:\n",
        "            return np.random.randint(self.action_size)\n",
        "        else:\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "            action_values = self.q_network(state)\n",
        "            action = np.argmax(action_values.cpu().data.numpy())\n",
        "            return action"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P3-UIm0fh3W"
      },
      "source": [
        "### 3. Executes episodes and train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NikAZhjNfsoi"
      },
      "source": [
        "We first define some paramters which are guiding the training process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJGrZry81pu4"
      },
      "source": [
        "MAX_EPISODES = 2000  # Max number of episodes to play\n",
        "MAX_STEPS = 1000     # Max steps allowed in a single episode/play\n",
        "\n",
        "# Epsilon schedule\n",
        "EPS_START = 1.0      # Default/starting value of eps\n",
        "EPS_DECAY = 0.999    # Epsilon decay rate\n",
        "EPS_MIN = 0.01       # Minimum epsilon "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezOn9IpKf17C"
      },
      "source": [
        "Then we start executing episodes and observe the mean score per episode.\n",
        "The environment is considered as solved if this score is above 200."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_EC7XLJ1slY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad110ade-36e5-4600-fcf0-faf2edc07235"
      },
      "source": [
        "# Get state and action sizes\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "print('State size: {}, action size: {}'.format(state_size, action_size))\n",
        "dqn_agent = DQNAgent(state_size, action_size, seed=0)\n",
        "start = time()\n",
        "\n",
        "# Maintain a list of last 100 scores\n",
        "scores_window = deque(maxlen=100)\n",
        "eps = EPS_START\n",
        "for episode in range(1, MAX_EPISODES + 1):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    for t in range(MAX_STEPS):\n",
        "        action = dqn_agent.act(state, eps)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        dqn_agent.step(state, action, reward, next_state, done)\n",
        "        state = next_state        \n",
        "        score += reward        \n",
        "        if done:\n",
        "            break\n",
        "            \n",
        "        eps = max(eps * EPS_DECAY, EPS_MIN)\n",
        "\n",
        "    scores_window.append(score)\n",
        "\n",
        "    if episode % 99 == 0:\n",
        "        mean_score = np.mean(scores_window)\n",
        "        print('Progress {}/{}, average score:{:.2f}'.format(episode, MAX_EPISODES, mean_score))\n",
        "\n",
        "    mean_score = np.mean(scores_window)\n",
        "    if mean_score >= 200:\n",
        "        print('\\rEnvironment solved in {} episodes, average score: {:.2f}'.format(episode, mean_score))\n",
        "        sys.stdout.flush()\n",
        "        break\n",
        "            \n",
        "end = time()    \n",
        "print('Took {} seconds'.format(end - start))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State size: 8, action size: 4\n",
            "Progress 99/2000, average score:-180.35\n",
            "Progress 198/2000, average score:-92.00\n",
            "Progress 297/2000, average score:-107.58\n",
            "Progress 396/2000, average score:-73.57\n",
            "Progress 495/2000, average score:-21.42\n",
            "Progress 594/2000, average score:1.12\n",
            "Progress 693/2000, average score:110.21\n",
            "Progress 792/2000, average score:169.26\n",
            "Progress 891/2000, average score:191.26\n",
            "Environment solved in 955 episodes, average score: 201.38\n",
            "Took 1670.806488752365 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd7QnYQRVUFc"
      },
      "source": [
        "### 4. Play epsiode and record it\n",
        "\n",
        "The following code enables Colab to record sessions (not needed when using executing code locally)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f3gVKzJoFss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8095071b-51e6-4877-d076-a4ccc1c3d472"
      },
      "source": [
        "!apt-get install -y xvfb x11-utils\n",
        "\n",
        "!pip install pyvirtualdisplay==0.2.* \\\n",
        "             PyOpenGL==3.1.* \\\n",
        "             PyOpenGL-accelerate==3.1.*\n",
        "\n",
        "!pip install gym[box2d]==0.17.*\n",
        "\n",
        "import pyvirtualdisplay\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
        "_ = _display.start()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay==0.2.* in /usr/local/lib/python3.6/dist-packages (0.2.5)\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.6/dist-packages (3.1.5)\n",
            "Requirement already satisfied: PyOpenGL-accelerate==3.1.* in /usr/local/lib/python3.6/dist-packages (3.1.5)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay==0.2.*) (0.3)\n",
            "Requirement already satisfied: gym[box2d]==0.17.* in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (1.19.4)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n",
            "Requirement already satisfied: box2d-py~=2.3.5; extra == \"box2d\" in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (2.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gki1NW8sVlyd"
      },
      "source": [
        "Use the trained model to play and record one episode. The recorded video will be stored into the `video`-subfolder on disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-KWsd02TRZq",
        "outputId": "2b18059f-f9af-4ca7-ff69-cadfb97df3ac"
      },
      "source": [
        "import time\n",
        "\n",
        "FPS = 25\n",
        "record_folder=\"video\"  \n",
        "\n",
        "env = gym.make('LunarLander-v2')\n",
        "env = gym.wrappers.Monitor(env, record_folder, force=True)\n",
        "\n",
        "state = env.reset()\n",
        "total_reward = 0.0\n",
        "\n",
        "while True:\n",
        "        start_ts = time.time()\n",
        "        env.render()\n",
        "        \n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        action_values = dqn_agent.q_network(state)\n",
        "        action = np.argmax(action_values.cpu().data.numpy())\n",
        "\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "            \n",
        "        delta = 1/FPS - (time.time() - start_ts)\n",
        "        if delta > 0:\n",
        "            time.sleep(delta)\n",
        "\n",
        "print(\"Total reward: %.2f\" % total_reward)\n",
        "env.close()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total reward: 270.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPk6moRMGNux"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}