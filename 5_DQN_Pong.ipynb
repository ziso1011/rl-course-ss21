{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "5_DQN_Pong.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pabair/rl-course-ss21/blob/main/5_DQN_Pong.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40Yb47zJQglm"
      },
      "source": [
        "#Â Deep Q-Network with Atari Pong\n",
        "\n",
        "This notebooks trains an agent to learn playing Atari Pong by using directly  pixel values as input.\n",
        "\n",
        "Note: The original source can be found [here](https://github.com/jorditorresBCN/Deep-Reinforcement-Learning-Explained/blob/master/DRL_15_16_17_DQN_Pong.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q40Fa7qM4_lE"
      },
      "source": [
        "## OpenAI Pong\n",
        "First, we load the environment for Pong and check the state and action space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA1Y5VCv20XZ"
      },
      "source": [
        "import gym\n",
        "import gym.spaces\n",
        "\n",
        "test_env = gym.make(\"PongNoFrameskip-v4\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QDaXip14JBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55844204-c85f-4113-e427-e94fc4ebfbdd"
      },
      "source": [
        "print(test_env.action_space.n)\n",
        "print(test_env.unwrapped.get_action_meanings())\n",
        "print(test_env.observation_space.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
            "(210, 160, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhmsqgrHikEl"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRcuJGVSQi6g"
      },
      "source": [
        "## OpenAI Gym Wrappers\n",
        "To effectively work with the environment, we define some helper methods for processing the image data that comes from the environment.\n",
        "\n",
        "Hint: You can savely skip this section while going through the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPi1lHINMuSu"
      },
      "source": [
        "# Taken from \n",
        "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/wrappers.py\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = collections.deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self._obs_buffer.clear()\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return ProcessFrame84.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        if frame.size == 210 * 160 * 3:\n",
        "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
        "        elif frame.size == 250 * 160 * 3:\n",
        "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
        "        else:\n",
        "            assert False, \"Unknown resolution.\"\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
        "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer\n",
        "\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], \n",
        "                                old_shape[0], old_shape[1]), dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "def make_env(env_name):\n",
        "    env = gym.make(env_name)\n",
        "    env = MaxAndSkipEnv(env)\n",
        "    env = FireResetEnv(env)\n",
        "    env = ProcessFrame84(env)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, 4)\n",
        "    return ScaledFloatFrame(env)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wznv9I1KR_I3"
      },
      "source": [
        "## The DQN model\n",
        "\n",
        "The neural network has three convolutional layer to understand the image data, which are followed by two fully connected layers. The output is a score for each of available actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6B8v-Qh5Ykk"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn       \n",
        "import torch.optim as optim \n",
        "\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4S1I9xWMkf3"
      },
      "source": [
        "# Taken from \n",
        "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/lib/dqn_model.py\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(4, 32, kernel_size=8, stride=4), # 4 previous frames\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1) # flatten the image to 1D\n",
        "        return self.fc(conv_out)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taYi5LZnIOqz",
        "outputId": "93da987c-e128-4137-bcc9-28aca09147a4"
      },
      "source": [
        "test_env = make_env(\"PongNoFrameskip-v4\")\n",
        "test_net = DQN(test_env.action_space.n).to(device)\n",
        "print(test_net)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=6, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhv3Yf-aW7UW"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb_f_onMXkpb"
      },
      "source": [
        "Import required modules and define the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGwHC9dyXoPd"
      },
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "\n",
        "MEAN_REWARD_BOUND = 19.0           \n",
        "\n",
        "gamma = 0.99                   \n",
        "batch_size = 32                \n",
        "replay_size = 10000            \n",
        "learning_rate = 1e-4           \n",
        "sync_target_frames = 1000      \n",
        "replay_start_size = 10000      \n",
        "\n",
        "eps_start=1.0\n",
        "eps_decay=.999985\n",
        "eps_min=0.02"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFaMmDKqYmo4"
      },
      "source": [
        "Experience replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y79CNYsjY4w0"
      },
      "source": [
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
        "\n",
        "class ExperienceReplay:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), np.array(next_states)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQDV04ktY3xs"
      },
      "source": [
        "Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdAKFiMWZw90"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, env, exp_buffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state = env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
        "\n",
        "        done_reward = None\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            state_a = np.array([self.state], copy=False)\n",
        "            state_v = torch.tensor(state_a).to(device)\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        new_state, reward, is_done, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipurwYpa6iKn",
        "outputId": "746b10ee-1add-4aa8-bd42-6a1594bc3966"
      },
      "source": [
        "import datetime\n",
        "print(\">>>Training starts at \",datetime.datetime.now())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>>Training starts at  2021-01-11 13:06:08.172844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgpmAtchZwM_"
      },
      "source": [
        "Main training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEoc2PWmM2mu",
        "outputId": "33924493-9bbc-4282-9618-14a6cd6a4e56"
      },
      "source": [
        "env = make_env(\"PongNoFrameskip-v4\")\n",
        "\n",
        "net = DQN(env.action_space.n).to(device)\n",
        "target_net = DQN(env.action_space.n).to(device)\n",
        " \n",
        "buffer = ExperienceReplay(replay_size)\n",
        "agent = Agent(env, buffer)\n",
        "\n",
        "epsilon = eps_start\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "total_rewards = []\n",
        "frame_idx = 0  \n",
        "\n",
        "best_mean_reward = None\n",
        "\n",
        "while True:\n",
        "        frame_idx += 1\n",
        "        epsilon = max(epsilon*eps_decay, eps_min)\n",
        "\n",
        "        reward = agent.play_step(net, epsilon, device=device)\n",
        "        if reward is not None:\n",
        "            total_rewards.append(reward)\n",
        "\n",
        "            mean_reward = np.mean(total_rewards[-100:])\n",
        "\n",
        "            print(\"%d:  %d games, mean reward %.3f, (epsilon %.2f)\" % (\n",
        "                frame_idx, len(total_rewards), mean_reward, epsilon))\n",
        "          \n",
        "\n",
        "            if best_mean_reward is None or best_mean_reward < mean_reward:\n",
        "                torch.save(net.state_dict(), \"PongNoFrameskip-v4\" + \"-best.dat\")\n",
        "                best_mean_reward = mean_reward\n",
        "                if best_mean_reward is not None:\n",
        "                    print(\"Best mean reward updated %.3f\" % (best_mean_reward))\n",
        "\n",
        "            if mean_reward > MEAN_REWARD_BOUND:\n",
        "                print(\"Solved in %d frames!\" % frame_idx)\n",
        "                break\n",
        "\n",
        "        if len(buffer) < replay_start_size:\n",
        "            continue\n",
        "\n",
        "        batch = buffer.sample(batch_size)\n",
        "        states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "        states_v = torch.tensor(states).to(device)\n",
        "        next_states_v = torch.tensor(next_states).to(device)\n",
        "        actions_v = torch.tensor(actions).to(device)\n",
        "        rewards_v = torch.tensor(rewards).to(device)\n",
        "        done_mask = torch.ByteTensor(dones).to(device)\n",
        "\n",
        "        state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        next_state_values = target_net(next_states_v).max(1)[0]\n",
        "\n",
        "        next_state_values[done_mask] = 0.0\n",
        "\n",
        "        next_state_values = next_state_values.detach()\n",
        "\n",
        "        expected_state_action_values = next_state_values * gamma + rewards_v\n",
        "\n",
        "        loss_t = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_t.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if frame_idx % sync_target_frames == 0:\n",
        "            target_net.load_state_dict(net.state_dict())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "979:  1 games, mean reward -20.000, (epsilon 0.99)\n",
            "Best mean reward updated -20.000\n",
            "2003:  2 games, mean reward -20.000, (epsilon 0.97)\n",
            "2885:  3 games, mean reward -20.333, (epsilon 0.96)\n",
            "3647:  4 games, mean reward -20.500, (epsilon 0.95)\n",
            "4691:  5 games, mean reward -20.400, (epsilon 0.93)\n",
            "5620:  6 games, mean reward -20.500, (epsilon 0.92)\n",
            "6484:  7 games, mean reward -20.429, (epsilon 0.91)\n",
            "7489:  8 games, mean reward -20.375, (epsilon 0.89)\n",
            "8428:  9 games, mean reward -20.444, (epsilon 0.88)\n",
            "9409:  10 games, mean reward -20.300, (epsilon 0.87)\n",
            "10316:  11 games, mean reward -20.273, (epsilon 0.86)\n",
            "11200:  12 games, mean reward -20.333, (epsilon 0.85)\n",
            "12144:  13 games, mean reward -20.385, (epsilon 0.83)\n",
            "12953:  14 games, mean reward -20.429, (epsilon 0.82)\n",
            "13771:  15 games, mean reward -20.467, (epsilon 0.81)\n",
            "14806:  16 games, mean reward -20.438, (epsilon 0.80)\n",
            "15718:  17 games, mean reward -20.471, (epsilon 0.79)\n",
            "16679:  18 games, mean reward -20.444, (epsilon 0.78)\n",
            "17501:  19 games, mean reward -20.474, (epsilon 0.77)\n",
            "18351:  20 games, mean reward -20.500, (epsilon 0.76)\n",
            "19268:  21 games, mean reward -20.476, (epsilon 0.75)\n",
            "20374:  22 games, mean reward -20.455, (epsilon 0.74)\n",
            "21345:  23 games, mean reward -20.391, (epsilon 0.73)\n",
            "22334:  24 games, mean reward -20.375, (epsilon 0.72)\n",
            "23364:  25 games, mean reward -20.280, (epsilon 0.70)\n",
            "24343:  26 games, mean reward -20.269, (epsilon 0.69)\n",
            "25165:  27 games, mean reward -20.296, (epsilon 0.69)\n",
            "25987:  28 games, mean reward -20.321, (epsilon 0.68)\n",
            "26841:  29 games, mean reward -20.310, (epsilon 0.67)\n",
            "27689:  30 games, mean reward -20.333, (epsilon 0.66)\n",
            "28479:  31 games, mean reward -20.355, (epsilon 0.65)\n",
            "29346:  32 games, mean reward -20.375, (epsilon 0.64)\n",
            "30690:  33 games, mean reward -20.303, (epsilon 0.63)\n",
            "31512:  34 games, mean reward -20.324, (epsilon 0.62)\n",
            "32538:  35 games, mean reward -20.314, (epsilon 0.61)\n",
            "33535:  36 games, mean reward -20.278, (epsilon 0.60)\n",
            "34734:  37 games, mean reward -20.216, (epsilon 0.59)\n",
            "35570:  38 games, mean reward -20.211, (epsilon 0.59)\n",
            "36476:  39 games, mean reward -20.231, (epsilon 0.58)\n",
            "37316:  40 games, mean reward -20.250, (epsilon 0.57)\n",
            "38078:  41 games, mean reward -20.268, (epsilon 0.56)\n",
            "39048:  42 games, mean reward -20.286, (epsilon 0.56)\n",
            "40009:  43 games, mean reward -20.279, (epsilon 0.55)\n",
            "40789:  44 games, mean reward -20.295, (epsilon 0.54)\n",
            "41611:  45 games, mean reward -20.311, (epsilon 0.54)\n",
            "42844:  46 games, mean reward -20.261, (epsilon 0.53)\n",
            "43724:  47 games, mean reward -20.277, (epsilon 0.52)\n",
            "44608:  48 games, mean reward -20.292, (epsilon 0.51)\n",
            "45430:  49 games, mean reward -20.306, (epsilon 0.51)\n",
            "46252:  50 games, mean reward -20.320, (epsilon 0.50)\n",
            "47213:  51 games, mean reward -20.333, (epsilon 0.49)\n",
            "48241:  52 games, mean reward -20.346, (epsilon 0.48)\n",
            "49199:  53 games, mean reward -20.340, (epsilon 0.48)\n",
            "50017:  54 games, mean reward -20.352, (epsilon 0.47)\n",
            "50973:  55 games, mean reward -20.345, (epsilon 0.47)\n",
            "51909:  56 games, mean reward -20.357, (epsilon 0.46)\n",
            "52731:  57 games, mean reward -20.368, (epsilon 0.45)\n",
            "53903:  58 games, mean reward -20.345, (epsilon 0.45)\n",
            "54750:  59 games, mean reward -20.356, (epsilon 0.44)\n",
            "55864:  60 games, mean reward -20.367, (epsilon 0.43)\n",
            "56887:  61 games, mean reward -20.377, (epsilon 0.43)\n",
            "58115:  62 games, mean reward -20.371, (epsilon 0.42)\n",
            "59292:  63 games, mean reward -20.381, (epsilon 0.41)\n",
            "60697:  64 games, mean reward -20.359, (epsilon 0.40)\n",
            "62067:  65 games, mean reward -20.323, (epsilon 0.39)\n",
            "63501:  66 games, mean reward -20.288, (epsilon 0.39)\n",
            "64822:  67 games, mean reward -20.299, (epsilon 0.38)\n",
            "66324:  68 games, mean reward -20.265, (epsilon 0.37)\n",
            "68167:  69 games, mean reward -20.217, (epsilon 0.36)\n",
            "69123:  70 games, mean reward -20.229, (epsilon 0.35)\n",
            "70669:  71 games, mean reward -20.183, (epsilon 0.35)\n",
            "72693:  72 games, mean reward -20.083, (epsilon 0.34)\n",
            "74222:  73 games, mean reward -20.082, (epsilon 0.33)\n",
            "75734:  74 games, mean reward -20.068, (epsilon 0.32)\n",
            "77696:  75 games, mean reward -20.027, (epsilon 0.31)\n",
            "79086:  76 games, mean reward -20.026, (epsilon 0.31)\n",
            "80291:  77 games, mean reward -20.026, (epsilon 0.30)\n",
            "81536:  78 games, mean reward -20.038, (epsilon 0.29)\n",
            "83003:  79 games, mean reward -20.000, (epsilon 0.29)\n",
            "84590:  80 games, mean reward -19.938, (epsilon 0.28)\n",
            "Best mean reward updated -19.938\n",
            "86096:  81 games, mean reward -19.914, (epsilon 0.27)\n",
            "Best mean reward updated -19.914\n",
            "87565:  82 games, mean reward -19.890, (epsilon 0.27)\n",
            "Best mean reward updated -19.890\n",
            "89165:  83 games, mean reward -19.855, (epsilon 0.26)\n",
            "Best mean reward updated -19.855\n",
            "90507:  84 games, mean reward -19.821, (epsilon 0.26)\n",
            "Best mean reward updated -19.821\n",
            "91691:  85 games, mean reward -19.835, (epsilon 0.25)\n",
            "93041:  86 games, mean reward -19.826, (epsilon 0.25)\n",
            "94638:  87 games, mean reward -19.805, (epsilon 0.24)\n",
            "Best mean reward updated -19.805\n",
            "96155:  88 games, mean reward -19.807, (epsilon 0.24)\n",
            "98085:  89 games, mean reward -19.764, (epsilon 0.23)\n",
            "Best mean reward updated -19.764\n",
            "99917:  90 games, mean reward -19.711, (epsilon 0.22)\n",
            "Best mean reward updated -19.711\n",
            "101708:  91 games, mean reward -19.670, (epsilon 0.22)\n",
            "Best mean reward updated -19.670\n",
            "103157:  92 games, mean reward -19.641, (epsilon 0.21)\n",
            "Best mean reward updated -19.641\n",
            "105147:  93 games, mean reward -19.559, (epsilon 0.21)\n",
            "Best mean reward updated -19.559\n",
            "107172:  94 games, mean reward -19.532, (epsilon 0.20)\n",
            "Best mean reward updated -19.532\n",
            "108728:  95 games, mean reward -19.526, (epsilon 0.20)\n",
            "Best mean reward updated -19.526\n",
            "110555:  96 games, mean reward -19.500, (epsilon 0.19)\n",
            "Best mean reward updated -19.500\n",
            "112507:  97 games, mean reward -19.454, (epsilon 0.18)\n",
            "Best mean reward updated -19.454\n",
            "114256:  98 games, mean reward -19.429, (epsilon 0.18)\n",
            "Best mean reward updated -19.429\n",
            "116354:  99 games, mean reward -19.374, (epsilon 0.17)\n",
            "Best mean reward updated -19.374\n",
            "118140:  100 games, mean reward -19.350, (epsilon 0.17)\n",
            "Best mean reward updated -19.350\n",
            "120118:  101 games, mean reward -19.320, (epsilon 0.17)\n",
            "Best mean reward updated -19.320\n",
            "121874:  102 games, mean reward -19.290, (epsilon 0.16)\n",
            "Best mean reward updated -19.290\n",
            "124223:  103 games, mean reward -19.220, (epsilon 0.16)\n",
            "Best mean reward updated -19.220\n",
            "125871:  104 games, mean reward -19.190, (epsilon 0.15)\n",
            "Best mean reward updated -19.190\n",
            "128289:  105 games, mean reward -19.140, (epsilon 0.15)\n",
            "Best mean reward updated -19.140\n",
            "130526:  106 games, mean reward -19.050, (epsilon 0.14)\n",
            "Best mean reward updated -19.050\n",
            "132876:  107 games, mean reward -18.970, (epsilon 0.14)\n",
            "Best mean reward updated -18.970\n",
            "134830:  108 games, mean reward -18.900, (epsilon 0.13)\n",
            "Best mean reward updated -18.900\n",
            "136796:  109 games, mean reward -18.810, (epsilon 0.13)\n",
            "Best mean reward updated -18.810\n",
            "138692:  110 games, mean reward -18.780, (epsilon 0.12)\n",
            "Best mean reward updated -18.780\n",
            "140637:  111 games, mean reward -18.730, (epsilon 0.12)\n",
            "Best mean reward updated -18.730\n",
            "142962:  112 games, mean reward -18.650, (epsilon 0.12)\n",
            "Best mean reward updated -18.650\n",
            "144921:  113 games, mean reward -18.590, (epsilon 0.11)\n",
            "Best mean reward updated -18.590\n",
            "147098:  114 games, mean reward -18.500, (epsilon 0.11)\n",
            "Best mean reward updated -18.500\n",
            "149129:  115 games, mean reward -18.450, (epsilon 0.11)\n",
            "Best mean reward updated -18.450\n",
            "150798:  116 games, mean reward -18.400, (epsilon 0.10)\n",
            "Best mean reward updated -18.400\n",
            "152780:  117 games, mean reward -18.300, (epsilon 0.10)\n",
            "Best mean reward updated -18.300\n",
            "155247:  118 games, mean reward -18.210, (epsilon 0.10)\n",
            "Best mean reward updated -18.210\n",
            "157476:  119 games, mean reward -18.120, (epsilon 0.09)\n",
            "Best mean reward updated -18.120\n",
            "159405:  120 games, mean reward -18.080, (epsilon 0.09)\n",
            "Best mean reward updated -18.080\n",
            "161899:  121 games, mean reward -17.990, (epsilon 0.09)\n",
            "Best mean reward updated -17.990\n",
            "164110:  122 games, mean reward -17.960, (epsilon 0.09)\n",
            "Best mean reward updated -17.960\n",
            "167090:  123 games, mean reward -17.860, (epsilon 0.08)\n",
            "Best mean reward updated -17.860\n",
            "169950:  124 games, mean reward -17.740, (epsilon 0.08)\n",
            "Best mean reward updated -17.740\n",
            "172912:  125 games, mean reward -17.630, (epsilon 0.07)\n",
            "Best mean reward updated -17.630\n",
            "175869:  126 games, mean reward -17.510, (epsilon 0.07)\n",
            "Best mean reward updated -17.510\n",
            "178785:  127 games, mean reward -17.390, (epsilon 0.07)\n",
            "Best mean reward updated -17.390\n",
            "181920:  128 games, mean reward -17.250, (epsilon 0.07)\n",
            "Best mean reward updated -17.250\n",
            "185303:  129 games, mean reward -17.090, (epsilon 0.06)\n",
            "Best mean reward updated -17.090\n",
            "187446:  130 games, mean reward -16.990, (epsilon 0.06)\n",
            "Best mean reward updated -16.990\n",
            "190245:  131 games, mean reward -16.830, (epsilon 0.06)\n",
            "Best mean reward updated -16.830\n",
            "193222:  132 games, mean reward -16.690, (epsilon 0.06)\n",
            "Best mean reward updated -16.690\n",
            "196235:  133 games, mean reward -16.610, (epsilon 0.05)\n",
            "Best mean reward updated -16.610\n",
            "199430:  134 games, mean reward -16.450, (epsilon 0.05)\n",
            "Best mean reward updated -16.450\n",
            "202925:  135 games, mean reward -16.230, (epsilon 0.05)\n",
            "Best mean reward updated -16.230\n",
            "205584:  136 games, mean reward -16.120, (epsilon 0.05)\n",
            "Best mean reward updated -16.120\n",
            "208131:  137 games, mean reward -16.030, (epsilon 0.04)\n",
            "Best mean reward updated -16.030\n",
            "210590:  138 games, mean reward -15.880, (epsilon 0.04)\n",
            "Best mean reward updated -15.880\n",
            "213502:  139 games, mean reward -15.720, (epsilon 0.04)\n",
            "Best mean reward updated -15.720\n",
            "216588:  140 games, mean reward -15.520, (epsilon 0.04)\n",
            "Best mean reward updated -15.520\n",
            "219162:  141 games, mean reward -15.390, (epsilon 0.04)\n",
            "Best mean reward updated -15.390\n",
            "221845:  142 games, mean reward -15.130, (epsilon 0.04)\n",
            "Best mean reward updated -15.130\n",
            "224859:  143 games, mean reward -14.880, (epsilon 0.03)\n",
            "Best mean reward updated -14.880\n",
            "227839:  144 games, mean reward -14.630, (epsilon 0.03)\n",
            "Best mean reward updated -14.630\n",
            "230702:  145 games, mean reward -14.340, (epsilon 0.03)\n",
            "Best mean reward updated -14.340\n",
            "233196:  146 games, mean reward -14.090, (epsilon 0.03)\n",
            "Best mean reward updated -14.090\n",
            "236044:  147 games, mean reward -13.790, (epsilon 0.03)\n",
            "Best mean reward updated -13.790\n",
            "239311:  148 games, mean reward -13.470, (epsilon 0.03)\n",
            "Best mean reward updated -13.470\n",
            "242749:  149 games, mean reward -13.240, (epsilon 0.03)\n",
            "Best mean reward updated -13.240\n",
            "245784:  150 games, mean reward -12.940, (epsilon 0.03)\n",
            "Best mean reward updated -12.940\n",
            "248909:  151 games, mean reward -12.630, (epsilon 0.02)\n",
            "Best mean reward updated -12.630\n",
            "251569:  152 games, mean reward -12.300, (epsilon 0.02)\n",
            "Best mean reward updated -12.300\n",
            "254506:  153 games, mean reward -11.960, (epsilon 0.02)\n",
            "Best mean reward updated -11.960\n",
            "258060:  154 games, mean reward -11.680, (epsilon 0.02)\n",
            "Best mean reward updated -11.680\n",
            "261359:  155 games, mean reward -11.470, (epsilon 0.02)\n",
            "Best mean reward updated -11.470\n",
            "265060:  156 games, mean reward -11.210, (epsilon 0.02)\n",
            "Best mean reward updated -11.210\n",
            "267650:  157 games, mean reward -10.890, (epsilon 0.02)\n",
            "Best mean reward updated -10.890\n",
            "270983:  158 games, mean reward -10.650, (epsilon 0.02)\n",
            "Best mean reward updated -10.650\n",
            "274403:  159 games, mean reward -10.420, (epsilon 0.02)\n",
            "Best mean reward updated -10.420\n",
            "277202:  160 games, mean reward -10.150, (epsilon 0.02)\n",
            "Best mean reward updated -10.150\n",
            "280433:  161 games, mean reward -9.860, (epsilon 0.02)\n",
            "Best mean reward updated -9.860\n",
            "283449:  162 games, mean reward -9.610, (epsilon 0.02)\n",
            "Best mean reward updated -9.610\n",
            "285994:  163 games, mean reward -9.310, (epsilon 0.02)\n",
            "Best mean reward updated -9.310\n",
            "288464:  164 games, mean reward -8.990, (epsilon 0.02)\n",
            "Best mean reward updated -8.990\n",
            "291143:  165 games, mean reward -8.700, (epsilon 0.02)\n",
            "Best mean reward updated -8.700\n",
            "293514:  166 games, mean reward -8.410, (epsilon 0.02)\n",
            "Best mean reward updated -8.410\n",
            "296166:  167 games, mean reward -8.120, (epsilon 0.02)\n",
            "Best mean reward updated -8.120\n",
            "298877:  168 games, mean reward -7.860, (epsilon 0.02)\n",
            "Best mean reward updated -7.860\n",
            "301858:  169 games, mean reward -7.670, (epsilon 0.02)\n",
            "Best mean reward updated -7.670\n",
            "305134:  170 games, mean reward -7.390, (epsilon 0.02)\n",
            "Best mean reward updated -7.390\n",
            "308301:  171 games, mean reward -7.180, (epsilon 0.02)\n",
            "Best mean reward updated -7.180\n",
            "310824:  172 games, mean reward -6.910, (epsilon 0.02)\n",
            "Best mean reward updated -6.910\n",
            "313379:  173 games, mean reward -6.590, (epsilon 0.02)\n",
            "Best mean reward updated -6.590\n",
            "315690:  174 games, mean reward -6.280, (epsilon 0.02)\n",
            "Best mean reward updated -6.280\n",
            "317740:  175 games, mean reward -5.930, (epsilon 0.02)\n",
            "Best mean reward updated -5.930\n",
            "319656:  176 games, mean reward -5.550, (epsilon 0.02)\n",
            "Best mean reward updated -5.550\n",
            "321580:  177 games, mean reward -5.170, (epsilon 0.02)\n",
            "Best mean reward updated -5.170\n",
            "323454:  178 games, mean reward -4.780, (epsilon 0.02)\n",
            "Best mean reward updated -4.780\n",
            "325508:  179 games, mean reward -4.440, (epsilon 0.02)\n",
            "Best mean reward updated -4.440\n",
            "327297:  180 games, mean reward -4.090, (epsilon 0.02)\n",
            "Best mean reward updated -4.090\n",
            "329239:  181 games, mean reward -3.730, (epsilon 0.02)\n",
            "Best mean reward updated -3.730\n",
            "330997:  182 games, mean reward -3.360, (epsilon 0.02)\n",
            "Best mean reward updated -3.360\n",
            "333011:  183 games, mean reward -3.010, (epsilon 0.02)\n",
            "Best mean reward updated -3.010\n",
            "334760:  184 games, mean reward -2.640, (epsilon 0.02)\n",
            "Best mean reward updated -2.640\n",
            "336983:  185 games, mean reward -2.280, (epsilon 0.02)\n",
            "Best mean reward updated -2.280\n",
            "339113:  186 games, mean reward -1.930, (epsilon 0.02)\n",
            "Best mean reward updated -1.930\n",
            "341281:  187 games, mean reward -1.620, (epsilon 0.02)\n",
            "Best mean reward updated -1.620\n",
            "343248:  188 games, mean reward -1.260, (epsilon 0.02)\n",
            "Best mean reward updated -1.260\n",
            "345037:  189 games, mean reward -0.910, (epsilon 0.02)\n",
            "Best mean reward updated -0.910\n",
            "346832:  190 games, mean reward -0.570, (epsilon 0.02)\n",
            "Best mean reward updated -0.570\n",
            "349236:  191 games, mean reward -0.270, (epsilon 0.02)\n",
            "Best mean reward updated -0.270\n",
            "351099:  192 games, mean reward 0.100, (epsilon 0.02)\n",
            "Best mean reward updated 0.100\n",
            "352838:  193 games, mean reward 0.420, (epsilon 0.02)\n",
            "Best mean reward updated 0.420\n",
            "354732:  194 games, mean reward 0.780, (epsilon 0.02)\n",
            "Best mean reward updated 0.780\n",
            "356550:  195 games, mean reward 1.150, (epsilon 0.02)\n",
            "Best mean reward updated 1.150\n",
            "358672:  196 games, mean reward 1.490, (epsilon 0.02)\n",
            "Best mean reward updated 1.490\n",
            "360791:  197 games, mean reward 1.810, (epsilon 0.02)\n",
            "Best mean reward updated 1.810\n",
            "363197:  198 games, mean reward 2.110, (epsilon 0.02)\n",
            "Best mean reward updated 2.110\n",
            "364897:  199 games, mean reward 2.450, (epsilon 0.02)\n",
            "Best mean reward updated 2.450\n",
            "366765:  200 games, mean reward 2.810, (epsilon 0.02)\n",
            "Best mean reward updated 2.810\n",
            "368742:  201 games, mean reward 3.140, (epsilon 0.02)\n",
            "Best mean reward updated 3.140\n",
            "370646:  202 games, mean reward 3.500, (epsilon 0.02)\n",
            "Best mean reward updated 3.500\n",
            "372398:  203 games, mean reward 3.840, (epsilon 0.02)\n",
            "Best mean reward updated 3.840\n",
            "374579:  204 games, mean reward 4.180, (epsilon 0.02)\n",
            "Best mean reward updated 4.180\n",
            "376534:  205 games, mean reward 4.500, (epsilon 0.02)\n",
            "Best mean reward updated 4.500\n",
            "378240:  206 games, mean reward 4.830, (epsilon 0.02)\n",
            "Best mean reward updated 4.830\n",
            "380091:  207 games, mean reward 5.120, (epsilon 0.02)\n",
            "Best mean reward updated 5.120\n",
            "381999:  208 games, mean reward 5.420, (epsilon 0.02)\n",
            "Best mean reward updated 5.420\n",
            "383715:  209 games, mean reward 5.740, (epsilon 0.02)\n",
            "Best mean reward updated 5.740\n",
            "385913:  210 games, mean reward 6.010, (epsilon 0.02)\n",
            "Best mean reward updated 6.010\n",
            "387836:  211 games, mean reward 6.340, (epsilon 0.02)\n",
            "Best mean reward updated 6.340\n",
            "389697:  212 games, mean reward 6.660, (epsilon 0.02)\n",
            "Best mean reward updated 6.660\n",
            "391597:  213 games, mean reward 6.980, (epsilon 0.02)\n",
            "Best mean reward updated 6.980\n",
            "393446:  214 games, mean reward 7.290, (epsilon 0.02)\n",
            "Best mean reward updated 7.290\n",
            "395121:  215 games, mean reward 7.650, (epsilon 0.02)\n",
            "Best mean reward updated 7.650\n",
            "396900:  216 games, mean reward 8.000, (epsilon 0.02)\n",
            "Best mean reward updated 8.000\n",
            "399029:  217 games, mean reward 8.280, (epsilon 0.02)\n",
            "Best mean reward updated 8.280\n",
            "400781:  218 games, mean reward 8.580, (epsilon 0.02)\n",
            "Best mean reward updated 8.580\n",
            "402581:  219 games, mean reward 8.890, (epsilon 0.02)\n",
            "Best mean reward updated 8.890\n",
            "404403:  220 games, mean reward 9.250, (epsilon 0.02)\n",
            "Best mean reward updated 9.250\n",
            "406344:  221 games, mean reward 9.540, (epsilon 0.02)\n",
            "Best mean reward updated 9.540\n",
            "408310:  222 games, mean reward 9.900, (epsilon 0.02)\n",
            "Best mean reward updated 9.900\n",
            "410448:  223 games, mean reward 10.170, (epsilon 0.02)\n",
            "Best mean reward updated 10.170\n",
            "412208:  224 games, mean reward 10.440, (epsilon 0.02)\n",
            "Best mean reward updated 10.440\n",
            "414170:  225 games, mean reward 10.670, (epsilon 0.02)\n",
            "Best mean reward updated 10.670\n",
            "415876:  226 games, mean reward 10.960, (epsilon 0.02)\n",
            "Best mean reward updated 10.960\n",
            "417853:  227 games, mean reward 11.230, (epsilon 0.02)\n",
            "Best mean reward updated 11.230\n",
            "420028:  228 games, mean reward 11.430, (epsilon 0.02)\n",
            "Best mean reward updated 11.430\n",
            "422021:  229 games, mean reward 11.630, (epsilon 0.02)\n",
            "Best mean reward updated 11.630\n",
            "424278:  230 games, mean reward 11.890, (epsilon 0.02)\n",
            "Best mean reward updated 11.890\n",
            "426603:  231 games, mean reward 12.090, (epsilon 0.02)\n",
            "Best mean reward updated 12.090\n",
            "428519:  232 games, mean reward 12.340, (epsilon 0.02)\n",
            "Best mean reward updated 12.340\n",
            "430775:  233 games, mean reward 12.600, (epsilon 0.02)\n",
            "Best mean reward updated 12.600\n",
            "433134:  234 games, mean reward 12.800, (epsilon 0.02)\n",
            "Best mean reward updated 12.800\n",
            "435233:  235 games, mean reward 12.920, (epsilon 0.02)\n",
            "Best mean reward updated 12.920\n",
            "437101:  236 games, mean reward 13.190, (epsilon 0.02)\n",
            "Best mean reward updated 13.190\n",
            "439270:  237 games, mean reward 13.410, (epsilon 0.02)\n",
            "Best mean reward updated 13.410\n",
            "441066:  238 games, mean reward 13.650, (epsilon 0.02)\n",
            "Best mean reward updated 13.650\n",
            "443042:  239 games, mean reward 13.880, (epsilon 0.02)\n",
            "Best mean reward updated 13.880\n",
            "444929:  240 games, mean reward 14.080, (epsilon 0.02)\n",
            "Best mean reward updated 14.080\n",
            "446772:  241 games, mean reward 14.360, (epsilon 0.02)\n",
            "Best mean reward updated 14.360\n",
            "448584:  242 games, mean reward 14.500, (epsilon 0.02)\n",
            "Best mean reward updated 14.500\n",
            "450824:  243 games, mean reward 14.580, (epsilon 0.02)\n",
            "Best mean reward updated 14.580\n",
            "452538:  244 games, mean reward 14.740, (epsilon 0.02)\n",
            "Best mean reward updated 14.740\n",
            "454550:  245 games, mean reward 14.820, (epsilon 0.02)\n",
            "Best mean reward updated 14.820\n",
            "456742:  246 games, mean reward 14.910, (epsilon 0.02)\n",
            "Best mean reward updated 14.910\n",
            "458550:  247 games, mean reward 15.020, (epsilon 0.02)\n",
            "Best mean reward updated 15.020\n",
            "460403:  248 games, mean reward 15.090, (epsilon 0.02)\n",
            "Best mean reward updated 15.090\n",
            "462262:  249 games, mean reward 15.260, (epsilon 0.02)\n",
            "Best mean reward updated 15.260\n",
            "464136:  250 games, mean reward 15.340, (epsilon 0.02)\n",
            "Best mean reward updated 15.340\n",
            "466147:  251 games, mean reward 15.400, (epsilon 0.02)\n",
            "Best mean reward updated 15.400\n",
            "468001:  252 games, mean reward 15.460, (epsilon 0.02)\n",
            "Best mean reward updated 15.460\n",
            "469853:  253 games, mean reward 15.500, (epsilon 0.02)\n",
            "Best mean reward updated 15.500\n",
            "471627:  254 games, mean reward 15.620, (epsilon 0.02)\n",
            "Best mean reward updated 15.620\n",
            "473326:  255 games, mean reward 15.810, (epsilon 0.02)\n",
            "Best mean reward updated 15.810\n",
            "475379:  256 games, mean reward 15.920, (epsilon 0.02)\n",
            "Best mean reward updated 15.920\n",
            "477360:  257 games, mean reward 15.970, (epsilon 0.02)\n",
            "Best mean reward updated 15.970\n",
            "479130:  258 games, mean reward 16.110, (epsilon 0.02)\n",
            "Best mean reward updated 16.110\n",
            "481176:  259 games, mean reward 16.250, (epsilon 0.02)\n",
            "Best mean reward updated 16.250\n",
            "483004:  260 games, mean reward 16.380, (epsilon 0.02)\n",
            "Best mean reward updated 16.380\n",
            "484934:  261 games, mean reward 16.480, (epsilon 0.02)\n",
            "Best mean reward updated 16.480\n",
            "486614:  262 games, mean reward 16.630, (epsilon 0.02)\n",
            "Best mean reward updated 16.630\n",
            "488358:  263 games, mean reward 16.730, (epsilon 0.02)\n",
            "Best mean reward updated 16.730\n",
            "490047:  264 games, mean reward 16.800, (epsilon 0.02)\n",
            "Best mean reward updated 16.800\n",
            "491831:  265 games, mean reward 16.880, (epsilon 0.02)\n",
            "Best mean reward updated 16.880\n",
            "494250:  266 games, mean reward 16.900, (epsilon 0.02)\n",
            "Best mean reward updated 16.900\n",
            "496019:  267 games, mean reward 17.010, (epsilon 0.02)\n",
            "Best mean reward updated 17.010\n",
            "497995:  268 games, mean reward 17.100, (epsilon 0.02)\n",
            "Best mean reward updated 17.100\n",
            "499707:  269 games, mean reward 17.280, (epsilon 0.02)\n",
            "Best mean reward updated 17.280\n",
            "501575:  270 games, mean reward 17.400, (epsilon 0.02)\n",
            "Best mean reward updated 17.400\n",
            "503289:  271 games, mean reward 17.560, (epsilon 0.02)\n",
            "Best mean reward updated 17.560\n",
            "505284:  272 games, mean reward 17.560, (epsilon 0.02)\n",
            "507009:  273 games, mean reward 17.640, (epsilon 0.02)\n",
            "Best mean reward updated 17.640\n",
            "508931:  274 games, mean reward 17.690, (epsilon 0.02)\n",
            "Best mean reward updated 17.690\n",
            "510805:  275 games, mean reward 17.700, (epsilon 0.02)\n",
            "Best mean reward updated 17.700\n",
            "512762:  276 games, mean reward 17.690, (epsilon 0.02)\n",
            "514687:  277 games, mean reward 17.680, (epsilon 0.02)\n",
            "516357:  278 games, mean reward 17.700, (epsilon 0.02)\n",
            "518320:  279 games, mean reward 17.700, (epsilon 0.02)\n",
            "520154:  280 games, mean reward 17.690, (epsilon 0.02)\n",
            "521943:  281 games, mean reward 17.710, (epsilon 0.02)\n",
            "Best mean reward updated 17.710\n",
            "523960:  282 games, mean reward 17.690, (epsilon 0.02)\n",
            "525965:  283 games, mean reward 17.690, (epsilon 0.02)\n",
            "527659:  284 games, mean reward 17.690, (epsilon 0.02)\n",
            "529558:  285 games, mean reward 17.720, (epsilon 0.02)\n",
            "Best mean reward updated 17.720\n",
            "531360:  286 games, mean reward 17.750, (epsilon 0.02)\n",
            "Best mean reward updated 17.750\n",
            "533164:  287 games, mean reward 17.820, (epsilon 0.02)\n",
            "Best mean reward updated 17.820\n",
            "534888:  288 games, mean reward 17.860, (epsilon 0.02)\n",
            "Best mean reward updated 17.860\n",
            "536821:  289 games, mean reward 17.850, (epsilon 0.02)\n",
            "538680:  290 games, mean reward 17.840, (epsilon 0.02)\n",
            "540393:  291 games, mean reward 17.900, (epsilon 0.02)\n",
            "Best mean reward updated 17.900\n",
            "542198:  292 games, mean reward 17.890, (epsilon 0.02)\n",
            "544062:  293 games, mean reward 17.870, (epsilon 0.02)\n",
            "545737:  294 games, mean reward 17.880, (epsilon 0.02)\n",
            "547575:  295 games, mean reward 17.870, (epsilon 0.02)\n",
            "549348:  296 games, mean reward 17.900, (epsilon 0.02)\n",
            "551043:  297 games, mean reward 17.930, (epsilon 0.02)\n",
            "Best mean reward updated 17.930\n",
            "552728:  298 games, mean reward 18.000, (epsilon 0.02)\n",
            "Best mean reward updated 18.000\n",
            "554498:  299 games, mean reward 17.990, (epsilon 0.02)\n",
            "556125:  300 games, mean reward 18.010, (epsilon 0.02)\n",
            "Best mean reward updated 18.010\n",
            "557933:  301 games, mean reward 18.030, (epsilon 0.02)\n",
            "Best mean reward updated 18.030\n",
            "559616:  302 games, mean reward 18.040, (epsilon 0.02)\n",
            "Best mean reward updated 18.040\n",
            "561497:  303 games, mean reward 18.010, (epsilon 0.02)\n",
            "563213:  304 games, mean reward 18.040, (epsilon 0.02)\n",
            "565015:  305 games, mean reward 18.040, (epsilon 0.02)\n",
            "567288:  306 games, mean reward 17.950, (epsilon 0.02)\n",
            "569048:  307 games, mean reward 17.970, (epsilon 0.02)\n",
            "571130:  308 games, mean reward 17.960, (epsilon 0.02)\n",
            "572938:  309 games, mean reward 17.960, (epsilon 0.02)\n",
            "575574:  310 games, mean reward 17.990, (epsilon 0.02)\n",
            "577398:  311 games, mean reward 18.010, (epsilon 0.02)\n",
            "579316:  312 games, mean reward 17.990, (epsilon 0.02)\n",
            "581477:  313 games, mean reward 17.970, (epsilon 0.02)\n",
            "583117:  314 games, mean reward 17.990, (epsilon 0.02)\n",
            "584817:  315 games, mean reward 17.990, (epsilon 0.02)\n",
            "586461:  316 games, mean reward 18.000, (epsilon 0.02)\n",
            "588089:  317 games, mean reward 18.040, (epsilon 0.02)\n",
            "589911:  318 games, mean reward 18.050, (epsilon 0.02)\n",
            "Best mean reward updated 18.050\n",
            "591782:  319 games, mean reward 18.060, (epsilon 0.02)\n",
            "Best mean reward updated 18.060\n",
            "593833:  320 games, mean reward 18.030, (epsilon 0.02)\n",
            "595463:  321 games, mean reward 18.060, (epsilon 0.02)\n",
            "597250:  322 games, mean reward 18.060, (epsilon 0.02)\n",
            "599046:  323 games, mean reward 18.070, (epsilon 0.02)\n",
            "Best mean reward updated 18.070\n",
            "600784:  324 games, mean reward 18.070, (epsilon 0.02)\n",
            "602812:  325 games, mean reward 18.100, (epsilon 0.02)\n",
            "Best mean reward updated 18.100\n",
            "604737:  326 games, mean reward 18.070, (epsilon 0.02)\n",
            "606366:  327 games, mean reward 18.100, (epsilon 0.02)\n",
            "608012:  328 games, mean reward 18.180, (epsilon 0.02)\n",
            "Best mean reward updated 18.180\n",
            "610023:  329 games, mean reward 18.190, (epsilon 0.02)\n",
            "Best mean reward updated 18.190\n",
            "611685:  330 games, mean reward 18.250, (epsilon 0.02)\n",
            "Best mean reward updated 18.250\n",
            "613313:  331 games, mean reward 18.310, (epsilon 0.02)\n",
            "Best mean reward updated 18.310\n",
            "615101:  332 games, mean reward 18.330, (epsilon 0.02)\n",
            "Best mean reward updated 18.330\n",
            "616781:  333 games, mean reward 18.370, (epsilon 0.02)\n",
            "Best mean reward updated 18.370\n",
            "618967:  334 games, mean reward 18.340, (epsilon 0.02)\n",
            "620672:  335 games, mean reward 18.400, (epsilon 0.02)\n",
            "Best mean reward updated 18.400\n",
            "622598:  336 games, mean reward 18.400, (epsilon 0.02)\n",
            "624228:  337 games, mean reward 18.480, (epsilon 0.02)\n",
            "Best mean reward updated 18.480\n",
            "625890:  338 games, mean reward 18.490, (epsilon 0.02)\n",
            "Best mean reward updated 18.490\n",
            "627529:  339 games, mean reward 18.520, (epsilon 0.02)\n",
            "Best mean reward updated 18.520\n",
            "629747:  340 games, mean reward 18.430, (epsilon 0.02)\n",
            "631749:  341 games, mean reward 18.400, (epsilon 0.02)\n",
            "633632:  342 games, mean reward 18.400, (epsilon 0.02)\n",
            "635524:  343 games, mean reward 18.440, (epsilon 0.02)\n",
            "637149:  344 games, mean reward 18.450, (epsilon 0.02)\n",
            "638918:  345 games, mean reward 18.480, (epsilon 0.02)\n",
            "640816:  346 games, mean reward 18.500, (epsilon 0.02)\n",
            "642482:  347 games, mean reward 18.510, (epsilon 0.02)\n",
            "644513:  348 games, mean reward 18.480, (epsilon 0.02)\n",
            "646823:  349 games, mean reward 18.390, (epsilon 0.02)\n",
            "648600:  350 games, mean reward 18.420, (epsilon 0.02)\n",
            "650225:  351 games, mean reward 18.470, (epsilon 0.02)\n",
            "651854:  352 games, mean reward 18.500, (epsilon 0.02)\n",
            "653479:  353 games, mean reward 18.530, (epsilon 0.02)\n",
            "Best mean reward updated 18.530\n",
            "655230:  354 games, mean reward 18.530, (epsilon 0.02)\n",
            "656968:  355 games, mean reward 18.530, (epsilon 0.02)\n",
            "658735:  356 games, mean reward 18.560, (epsilon 0.02)\n",
            "Best mean reward updated 18.560\n",
            "660754:  357 games, mean reward 18.570, (epsilon 0.02)\n",
            "Best mean reward updated 18.570\n",
            "662628:  358 games, mean reward 18.550, (epsilon 0.02)\n",
            "664815:  359 games, mean reward 18.560, (epsilon 0.02)\n",
            "666596:  360 games, mean reward 18.560, (epsilon 0.02)\n",
            "668312:  361 games, mean reward 18.580, (epsilon 0.02)\n",
            "Best mean reward updated 18.580\n",
            "670333:  362 games, mean reward 18.540, (epsilon 0.02)\n",
            "672133:  363 games, mean reward 18.540, (epsilon 0.02)\n",
            "673884:  364 games, mean reward 18.540, (epsilon 0.02)\n",
            "675552:  365 games, mean reward 18.550, (epsilon 0.02)\n",
            "677180:  366 games, mean reward 18.630, (epsilon 0.02)\n",
            "Best mean reward updated 18.630\n",
            "678912:  367 games, mean reward 18.630, (epsilon 0.02)\n",
            "680700:  368 games, mean reward 18.650, (epsilon 0.02)\n",
            "Best mean reward updated 18.650\n",
            "682442:  369 games, mean reward 18.640, (epsilon 0.02)\n",
            "684190:  370 games, mean reward 18.650, (epsilon 0.02)\n",
            "686566:  371 games, mean reward 18.500, (epsilon 0.02)\n",
            "689187:  372 games, mean reward 18.470, (epsilon 0.02)\n",
            "690829:  373 games, mean reward 18.480, (epsilon 0.02)\n",
            "692678:  374 games, mean reward 18.490, (epsilon 0.02)\n",
            "694623:  375 games, mean reward 18.470, (epsilon 0.02)\n",
            "696248:  376 games, mean reward 18.510, (epsilon 0.02)\n",
            "698219:  377 games, mean reward 18.520, (epsilon 0.02)\n",
            "700053:  378 games, mean reward 18.510, (epsilon 0.02)\n",
            "701981:  379 games, mean reward 18.530, (epsilon 0.02)\n",
            "703864:  380 games, mean reward 18.500, (epsilon 0.02)\n",
            "705552:  381 games, mean reward 18.500, (epsilon 0.02)\n",
            "707424:  382 games, mean reward 18.520, (epsilon 0.02)\n",
            "709436:  383 games, mean reward 18.500, (epsilon 0.02)\n",
            "711347:  384 games, mean reward 18.490, (epsilon 0.02)\n",
            "713319:  385 games, mean reward 18.490, (epsilon 0.02)\n",
            "715107:  386 games, mean reward 18.500, (epsilon 0.02)\n",
            "716754:  387 games, mean reward 18.510, (epsilon 0.02)\n",
            "718468:  388 games, mean reward 18.510, (epsilon 0.02)\n",
            "720110:  389 games, mean reward 18.540, (epsilon 0.02)\n",
            "721735:  390 games, mean reward 18.570, (epsilon 0.02)\n",
            "723723:  391 games, mean reward 18.540, (epsilon 0.02)\n",
            "725467:  392 games, mean reward 18.540, (epsilon 0.02)\n",
            "727294:  393 games, mean reward 18.550, (epsilon 0.02)\n",
            "728950:  394 games, mean reward 18.560, (epsilon 0.02)\n",
            "730600:  395 games, mean reward 18.600, (epsilon 0.02)\n",
            "732297:  396 games, mean reward 18.600, (epsilon 0.02)\n",
            "734112:  397 games, mean reward 18.590, (epsilon 0.02)\n",
            "735737:  398 games, mean reward 18.600, (epsilon 0.02)\n",
            "737628:  399 games, mean reward 18.570, (epsilon 0.02)\n",
            "739663:  400 games, mean reward 18.530, (epsilon 0.02)\n",
            "741823:  401 games, mean reward 18.480, (epsilon 0.02)\n",
            "743594:  402 games, mean reward 18.470, (epsilon 0.02)\n",
            "745522:  403 games, mean reward 18.470, (epsilon 0.02)\n",
            "747525:  404 games, mean reward 18.460, (epsilon 0.02)\n",
            "749216:  405 games, mean reward 18.500, (epsilon 0.02)\n",
            "751024:  406 games, mean reward 18.560, (epsilon 0.02)\n",
            "753009:  407 games, mean reward 18.540, (epsilon 0.02)\n",
            "755013:  408 games, mean reward 18.560, (epsilon 0.02)\n",
            "756743:  409 games, mean reward 18.560, (epsilon 0.02)\n",
            "758395:  410 games, mean reward 18.630, (epsilon 0.02)\n",
            "760144:  411 games, mean reward 18.620, (epsilon 0.02)\n",
            "762106:  412 games, mean reward 18.620, (epsilon 0.02)\n",
            "763736:  413 games, mean reward 18.680, (epsilon 0.02)\n",
            "Best mean reward updated 18.680\n",
            "765430:  414 games, mean reward 18.680, (epsilon 0.02)\n",
            "767258:  415 games, mean reward 18.670, (epsilon 0.02)\n",
            "768961:  416 games, mean reward 18.670, (epsilon 0.02)\n",
            "770654:  417 games, mean reward 18.670, (epsilon 0.02)\n",
            "772345:  418 games, mean reward 18.680, (epsilon 0.02)\n",
            "774189:  419 games, mean reward 18.660, (epsilon 0.02)\n",
            "775950:  420 games, mean reward 18.690, (epsilon 0.02)\n",
            "Best mean reward updated 18.690\n",
            "777737:  421 games, mean reward 18.680, (epsilon 0.02)\n",
            "779365:  422 games, mean reward 18.700, (epsilon 0.02)\n",
            "Best mean reward updated 18.700\n",
            "781129:  423 games, mean reward 18.710, (epsilon 0.02)\n",
            "Best mean reward updated 18.710\n",
            "783818:  424 games, mean reward 18.630, (epsilon 0.02)\n",
            "785454:  425 games, mean reward 18.650, (epsilon 0.02)\n",
            "787344:  426 games, mean reward 18.650, (epsilon 0.02)\n",
            "788978:  427 games, mean reward 18.650, (epsilon 0.02)\n",
            "790816:  428 games, mean reward 18.630, (epsilon 0.02)\n",
            "792540:  429 games, mean reward 18.650, (epsilon 0.02)\n",
            "794243:  430 games, mean reward 18.640, (epsilon 0.02)\n",
            "795954:  431 games, mean reward 18.630, (epsilon 0.02)\n",
            "797741:  432 games, mean reward 18.620, (epsilon 0.02)\n",
            "799374:  433 games, mean reward 18.630, (epsilon 0.02)\n",
            "801103:  434 games, mean reward 18.700, (epsilon 0.02)\n",
            "802804:  435 games, mean reward 18.700, (epsilon 0.02)\n",
            "804622:  436 games, mean reward 18.700, (epsilon 0.02)\n",
            "806248:  437 games, mean reward 18.700, (epsilon 0.02)\n",
            "808022:  438 games, mean reward 18.690, (epsilon 0.02)\n",
            "810011:  439 games, mean reward 18.620, (epsilon 0.02)\n",
            "811678:  440 games, mean reward 18.720, (epsilon 0.02)\n",
            "Best mean reward updated 18.720\n",
            "813523:  441 games, mean reward 18.740, (epsilon 0.02)\n",
            "Best mean reward updated 18.740\n",
            "815294:  442 games, mean reward 18.740, (epsilon 0.02)\n",
            "817421:  443 games, mean reward 18.730, (epsilon 0.02)\n",
            "819389:  444 games, mean reward 18.670, (epsilon 0.02)\n",
            "821018:  445 games, mean reward 18.690, (epsilon 0.02)\n",
            "822646:  446 games, mean reward 18.720, (epsilon 0.02)\n",
            "824544:  447 games, mean reward 18.660, (epsilon 0.02)\n",
            "826226:  448 games, mean reward 18.710, (epsilon 0.02)\n",
            "828275:  449 games, mean reward 18.770, (epsilon 0.02)\n",
            "Best mean reward updated 18.770\n",
            "830183:  450 games, mean reward 18.750, (epsilon 0.02)\n",
            "832012:  451 games, mean reward 18.710, (epsilon 0.02)\n",
            "833639:  452 games, mean reward 18.710, (epsilon 0.02)\n",
            "835327:  453 games, mean reward 18.710, (epsilon 0.02)\n",
            "836973:  454 games, mean reward 18.730, (epsilon 0.02)\n",
            "838599:  455 games, mean reward 18.740, (epsilon 0.02)\n",
            "840225:  456 games, mean reward 18.760, (epsilon 0.02)\n",
            "842100:  457 games, mean reward 18.770, (epsilon 0.02)\n",
            "844347:  458 games, mean reward 18.720, (epsilon 0.02)\n",
            "846329:  459 games, mean reward 18.670, (epsilon 0.02)\n",
            "847956:  460 games, mean reward 18.690, (epsilon 0.02)\n",
            "849855:  461 games, mean reward 18.670, (epsilon 0.02)\n",
            "851647:  462 games, mean reward 18.700, (epsilon 0.02)\n",
            "853393:  463 games, mean reward 18.700, (epsilon 0.02)\n",
            "855021:  464 games, mean reward 18.710, (epsilon 0.02)\n",
            "856776:  465 games, mean reward 18.700, (epsilon 0.02)\n",
            "858498:  466 games, mean reward 18.690, (epsilon 0.02)\n",
            "860574:  467 games, mean reward 18.670, (epsilon 0.02)\n",
            "862267:  468 games, mean reward 18.680, (epsilon 0.02)\n",
            "864205:  469 games, mean reward 18.670, (epsilon 0.02)\n",
            "866019:  470 games, mean reward 18.660, (epsilon 0.02)\n",
            "868042:  471 games, mean reward 18.740, (epsilon 0.02)\n",
            "870034:  472 games, mean reward 18.810, (epsilon 0.02)\n",
            "Best mean reward updated 18.810\n",
            "872042:  473 games, mean reward 18.760, (epsilon 0.02)\n",
            "874307:  474 games, mean reward 18.740, (epsilon 0.02)\n",
            "876117:  475 games, mean reward 18.760, (epsilon 0.02)\n",
            "878089:  476 games, mean reward 18.720, (epsilon 0.02)\n",
            "879966:  477 games, mean reward 18.730, (epsilon 0.02)\n",
            "881737:  478 games, mean reward 18.740, (epsilon 0.02)\n",
            "883522:  479 games, mean reward 18.740, (epsilon 0.02)\n",
            "885189:  480 games, mean reward 18.780, (epsilon 0.02)\n",
            "886817:  481 games, mean reward 18.790, (epsilon 0.02)\n",
            "888628:  482 games, mean reward 18.790, (epsilon 0.02)\n",
            "890274:  483 games, mean reward 18.840, (epsilon 0.02)\n",
            "Best mean reward updated 18.840\n",
            "891946:  484 games, mean reward 18.850, (epsilon 0.02)\n",
            "Best mean reward updated 18.850\n",
            "893709:  485 games, mean reward 18.860, (epsilon 0.02)\n",
            "Best mean reward updated 18.860\n",
            "895481:  486 games, mean reward 18.850, (epsilon 0.02)\n",
            "897105:  487 games, mean reward 18.850, (epsilon 0.02)\n",
            "898892:  488 games, mean reward 18.850, (epsilon 0.02)\n",
            "900701:  489 games, mean reward 18.820, (epsilon 0.02)\n",
            "902330:  490 games, mean reward 18.820, (epsilon 0.02)\n",
            "904021:  491 games, mean reward 18.850, (epsilon 0.02)\n",
            "905778:  492 games, mean reward 18.860, (epsilon 0.02)\n",
            "907659:  493 games, mean reward 18.850, (epsilon 0.02)\n",
            "909396:  494 games, mean reward 18.840, (epsilon 0.02)\n",
            "911026:  495 games, mean reward 18.840, (epsilon 0.02)\n",
            "913028:  496 games, mean reward 18.810, (epsilon 0.02)\n",
            "914890:  497 games, mean reward 18.810, (epsilon 0.02)\n",
            "916618:  498 games, mean reward 18.800, (epsilon 0.02)\n",
            "918362:  499 games, mean reward 18.830, (epsilon 0.02)\n",
            "920054:  500 games, mean reward 18.860, (epsilon 0.02)\n",
            "921895:  501 games, mean reward 18.900, (epsilon 0.02)\n",
            "Best mean reward updated 18.900\n",
            "923602:  502 games, mean reward 18.910, (epsilon 0.02)\n",
            "Best mean reward updated 18.910\n",
            "925256:  503 games, mean reward 18.950, (epsilon 0.02)\n",
            "Best mean reward updated 18.950\n",
            "927101:  504 games, mean reward 18.950, (epsilon 0.02)\n",
            "928938:  505 games, mean reward 18.930, (epsilon 0.02)\n",
            "930656:  506 games, mean reward 18.950, (epsilon 0.02)\n",
            "932302:  507 games, mean reward 18.990, (epsilon 0.02)\n",
            "Best mean reward updated 18.990\n",
            "934052:  508 games, mean reward 19.010, (epsilon 0.02)\n",
            "Best mean reward updated 19.010\n",
            "Solved in 934052 frames!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZPkszw66cmO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a5191ff-1348-42cc-e9e4-a22bc9a75482"
      },
      "source": [
        "print(\">>>Training ends at \", datetime.datetime.now())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">>>Training ends at  2021-01-11 15:46:32.906726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p0jvxoC3m5W"
      },
      "source": [
        "## Using the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLEfbkKl6AZV"
      },
      "source": [
        "import gym\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "\n",
        "import collections\n",
        "\n",
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "FPS = 25"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m0Vm4Yp91ZI"
      },
      "source": [
        "Turn on image rendering on Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgpHXywd5SyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "729eb1ca-b085-47a6-9a2a-80a6887e1eef"
      },
      "source": [
        "# Taken from \n",
        "# https://towardsdatascience.com/rendering-openai-gym-envs-on-binder-and-google-colab-536f99391cc7\n",
        "\n",
        "!apt-get install -y xvfb x11-utils\n",
        "\n",
        "!pip install pyvirtualdisplay==0.2.* \\\n",
        "             PyOpenGL==3.1.* \\\n",
        "             PyOpenGL-accelerate==3.1.*\n",
        "\n",
        "!pip install gym[box2d]==0.17.*\n",
        "\n",
        "import pyvirtualdisplay\n",
        "\n",
        "_display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
        "_ = _display.start()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libxxf86dga1\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  libxxf86dga1 x11-utils xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 993 kB of archives.\n",
            "After this operation, 2,981 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 993 kB in 0s (9,277 kB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "(Reading database ... 145483 files and directories currently installed.)\n",
            "Preparing to unpack .../libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../x11-utils_7.7+3build1_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+3build1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Setting up x11-utils (7.7+3build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting pyvirtualdisplay==0.2.*\n",
            "  Downloading https://files.pythonhosted.org/packages/69/ec/8221a07850d69fa3c57c02e526edd23d18c7c05d58ed103e3b19172757c1/PyVirtualDisplay-0.2.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.6/dist-packages (3.1.5)\n",
            "Collecting PyOpenGL-accelerate==3.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/3c/f42a62b7784c04b20f8b88d6c8ad04f4f20b0767b721102418aad94d8389/PyOpenGL-accelerate-3.1.5.tar.gz (538kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 542kB 13.1MB/s \n",
            "\u001b[?25hCollecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: PyOpenGL-accelerate\n",
            "  Building wheel for PyOpenGL-accelerate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyOpenGL-accelerate: filename=PyOpenGL_accelerate-3.1.5-cp36-cp36m-linux_x86_64.whl size=1593658 sha256=f76120cbde6c52bb2dc46b72ec9290e823fd47b2567e377482d6b97dfbd7ae85\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/21/77/99670ceca25fddb3c2b60a7ae44644b8253d1006e8ec417bcc\n",
            "Successfully built PyOpenGL-accelerate\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay, PyOpenGL-accelerate\n",
            "Successfully installed EasyProcess-0.3 PyOpenGL-accelerate-3.1.5 pyvirtualdisplay-0.2.5\n",
            "Requirement already satisfied: gym[box2d]==0.17.* in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (1.19.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n",
            "Collecting box2d-py~=2.3.5; extra == \"box2d\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/bd/6cdc3fd994b0649dcf5d9bad85bd9e26172308bbe9a421bfc6fdbf5081a6/box2d_py-2.3.8-cp36-cp36m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |ââââââââââââââââââââââââââââââââ| 450kB 13.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.16.0)\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL1ompt3VFZT"
      },
      "source": [
        "Play one epsiode and record it into the \"video\"-folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvN4S8R53mJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f1cc106-8ca0-4e3d-8e0a-262ae3862264"
      },
      "source": [
        "# Taken (partially) from \n",
        "# https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On/blob/master/Chapter06/03_dqn_play.py\n",
        "\n",
        "\n",
        "model='PongNoFrameskip-v4-best.dat'\n",
        "record_folder=\"video\"  \n",
        "visualize=True\n",
        "\n",
        "env = make_env(DEFAULT_ENV_NAME)\n",
        "if record_folder:\n",
        "        env = gym.wrappers.Monitor(env, record_folder, force=True)\n",
        "net = DQN(env.action_space.n)\n",
        "net.load_state_dict(torch.load(model, map_location=lambda storage, loc: storage))\n",
        "\n",
        "state = env.reset()\n",
        "total_reward = 0.0\n",
        "\n",
        "while True:\n",
        "        start_ts = time.time()\n",
        "        if visualize:\n",
        "            env.render()\n",
        "        state_v = torch.tensor(np.array([state], copy=False))\n",
        "        q_vals = net(state_v).data.numpy()[0]\n",
        "        action = np.argmax(q_vals)\n",
        "        \n",
        "        state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "        if visualize:\n",
        "            delta = 1/FPS - (time.time() - start_ts)\n",
        "            if delta > 0:\n",
        "                time.sleep(delta)\n",
        "print(\"Total reward: %.2f\" % total_reward)\n",
        "\n",
        "if record_folder:\n",
        "        env.close()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total reward: 10.00\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}